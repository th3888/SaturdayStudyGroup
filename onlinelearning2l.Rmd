---
title: "オンライン機械学習 勉強会"
subtitle: "chapter 4 発展"
date: "`r format(Sys.time(),'%Y/%m/%d')`"
output:
  revealjs::revealjs_presentation:
    reveal_option:
      slideNumber: true
    #pandoc_args: [
    #  '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
    #]
    self_contained: True
    center: True
    theme: sky
---


# 4.1 高精度なオンライン学習

```{=html}
<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(revealjs)
library(ggplot2)
library(dplyr)
```

##
オンライン学習の枠組み

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}<E$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}+y^{(t)}\alpha{\bf A}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. **end for**

ここで、$\alpha, E$は$\alpha >0, E \ge0$を満たす実数、${\bf A}\in R^{m×m}$は半正定値行列

##

- 入力の次元$m$が大きい場合、半正定値行列を格納するために$m^{2}/2$個のパラメータが必要で、これとベクトルの積の処理は$O(m^3)$となり、計算負荷が大きい

- この問題を解決するために$\bf A$として対角行列を使うケースがほとんど

- この時の計算量は$O(m)$なので手軽になる


## 4.1.1 パーセプトロン

##
- パーセプトロンの更新則をオンライン学習の枠組みで考えると、$E=0, \alpha=1, {\bf A}={\bf I}$に対応する

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+y{\bf x}
\end{equation}

- パーセプトロンはどのように間違っても全ての特徴に対して同じ更新幅を用いる

##

- 5.2節で、パーセプトロンが線形分離可能な入力に対して有限回の更新で分類可能な重みベクトルを見つけられ、線形分離不可能な入力に対しても多くの重みベクトルを見つけられることの証明を扱う

- 改良版としては、分類時に全てのステップの重みベクトルの平均を用いる平均化パーセプトロンがある

- 平均化パーセプトロンの効率的な計算に関しては6.2.1項で扱う

## 4.1.2 Passive Aggressive(PA)

##

- PAでは、SVMで用いたヒンジ損失を使い以下の最適化問題を解く

\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})=0
\end{equation}

- これをラグランジュ未定乗数法で解く
\begin{equation}
L({\bf w}, \tau)=\frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+\tau (1-y^{(t)}{\bf w}^{(T)}{\bf x}^{(t)})
\end{equation}

\begin{equation}
{\bf w}={\bf w}^{(t)}+\tau y^{(t)}x^{(t)}
\end{equation}

## 
- 得られた${\bf w}$をラグランジュ関数に代入する