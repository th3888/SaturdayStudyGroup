---
title: "オンライン機械学習 勉強会"
subtitle: "chapter 4 発展"
date: "`r format(Sys.time(),'%Y/%m/%d')`"
output:
  revealjs::revealjs_presentation:
    reveal_option:
      slideNumber: true
    #pandoc_args: [
    #  '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
    #]
    self_contained: True
    center: True
    theme: sky
---


# 4.1 高精度なオンライン学習

```{=html}
<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(revealjs)
library(ggplot2)
library(dplyr)
```

##
オンライン学習の枠組み

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}<E$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}+y^{(t)}\alpha{\bf A}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. **end for**

ここで、$\alpha, E$は$\alpha >0, E \ge0$を満たす実数、${\bf A}\in R^{m×m}$は半正定値行列

##

- 入力の次元$m$が大きい場合、半正定値行列を格納するために$m^{2}/2$個のパラメータが必要で、これとベクトルの積の処理は$O(m^3)$となり、計算負荷が大きい

- この問題を解決するために$\bf A$として対角行列を使うケースがほとんど

- この時の計算量は$O(m)$なので手軽になる


## 4.1.1 パーセプトロン

##
- パーセプトロンの更新則をオンライン学習の枠組みで考えると、$E=0, \alpha=1, {\bf A}={\bf I}$に対応する

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+y{\bf x}
\end{equation}

- パーセプトロンはどのように間違っても全ての特徴に対して同じ更新幅を用いる

##

- 5.2節で、パーセプトロンが線形分離可能な入力に対して有限回の更新で分類可能な重みベクトルを見つけられ、線形分離不可能な入力に対しても多くの重みベクトルを見つけられることの証明を扱う

- 改良版としては、分類時に全てのステップの重みベクトルの平均を用いる平均化パーセプトロンがある

- 平均化パーセプトロンの効率的な計算に関しては6.2.1項で扱う

## 4.1.2 Passive Aggressive(PA)

##

- 原論文はJMLR Crammer et al.(2006)

- PAでは、SVMで用いたヒンジ損失を使い以下の最適化問題を解く

\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})=\max(0, 1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)})=0
\end{equation}

- ヒンジ損失が$0$、すなわち$y^{(t)}{\bf w}^{T}{\bf x}^{(t)}>1$のとき、重みベクトルは更新されない(passive)

- $y^{(t)}{\bf w}^{T}{\bf x}^{(t)}<1$のとき、ヒンジ損失が$0$になるように重みベクトルを更新する(aggressive)

##

- aggressiveの場合、上の最適化問題をラグランジュ未定乗数法で解く
\begin{equation}
L({\bf w}, \tau)=\frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+\tau (1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)})
\end{equation}

\begin{equation}
{\bf w}={\bf w}^{(t)}+\tau y^{(t)}x^{(t)}
\end{equation}

- 得られた${\bf w}$をラグランジュ関数に代入する
\begin{equation}
L(\tau)=-\frac{1}{2}\tau^{2}\|{\bf x}^{(t)}\|^2+\tau(1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)})
\end{equation}

##

- これを$\tau$について偏微分し、最適化問題を解く
\begin{equation}
\frac{\partial L}{\partial \tau}=-\tau \|{\bf x}^{(t)}\|^{2}+(1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)})=0
\end{equation}

\begin{equation}
\tau=\frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}^{t}\|^{2}}
\end{equation}

- つまり、aggressiveの時の更新則は以下のようになる
\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+\frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}}y^{(t)}x^{(t)}
\end{equation}

##

- PAアルゴリズムの更新則では、ノイズのあるデータに大きく影響を受けたパラメータ更新をしてしまう

- この問題に対処するためにヒンジ損失についての制約を緩和することを考える

- SVMにおけるソフトマージン拡張と同様のアイデアを用いる

## 

- PA-Ⅰアルゴリズム
\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+C\xi
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})\le\xi, \xi\ge0
\end{equation}

- PA-Ⅱアルゴリズム

\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+C{\xi}^2
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})\le\xi
\end{equation}


ここで、$C$はスラック変数$\xi$の影響をコントロールする正のパラメータ


## 
- 通常のPAアルゴリズムと同様にこれらもラグランジュ法を使って解くことができるので、まずはPA-Ⅰから見ていく

\begin{equation}
L({\bf w}, \xi, \tau, \lambda)=\frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+\tau (1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)})+\xi(C-\tau-\lambda)
\end{equation}

- $w$についてのfirst order conditionを調べると、通常のPAアルゴリズムと同様の更新則を得る

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+\tau y^{(t)}x^{(t)}
\end{equation}

##

- $\xi(C-\tau-\lambda)$については、非ゼロの場合いくらでも$-\infty$に近づけられる

- 双対問題を考えた時最大化をする必要があるので、$\xi(C-\tau-\lambda)=0$についてのみ考える

- ここで、$\lambda\ge0$なので、$\tau\le C$が条件となる

##

- もし、$1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}/\|{\bf x}\|^{2}\le C$ならば

\begin{equation}
\tau=\frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}}
\end{equation}

- 次に$1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}/\|{\bf x}\|^{2}> C$の場合を考える

- この条件を書き直すと

\begin{equation}
C\|{\bf x}\|^{2} < 1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}
\end{equation}

## 

- また、制約条件より

\begin{equation}
1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}\le \xi 
\end{equation}

- 更新則より

\begin{equation}
1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}-\tau \|{\bf x}\|^{2}\le \xi 
\end{equation}

- これらにより以下が導ける
\begin{equation}
C\|{\bf x}\|^{2}-\tau \|{\bf x}\|^{2}< \xi 
\end{equation}

## 
- これと$\tau\le C$より$\xi>0$で、KKT条件より$\xi \lambda=0$なので$\lambda=0$

- よって、このときは$\tau = C$となり、更新則は以下のようになる

\begin{equation}
\tau = \min(C, \frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}})
\end{equation}

- つまり、更新幅がパラメータ$C$を超える場合は$C$でクリップされる

## 

- PA-Ⅱに関しても同様にラグランジュ法で解く

- PA-Ⅱのラグランジュ関数に関して${\bf w}, \xi$についてのFOCを調べて、得られた${\bf w}, \xi$をラグランジュ関数に代入し、$\tau$についてのFOCをとると、以下が得られる

\begin{equation}
\tau = \frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}+\frac{1}{2C}}
\end{equation}

# Confidence Weighted Learning (CW)

## 

- ICMLの Crammer et al. (2008) で提案

- 各重みについて、${\bf w}\sim \mathcal N({\bf \mu}, \Sigma)$であると考える

- 共分散行列$\Sigma\in {\mathbb R}^{m×m}$は対角成分を${\bf \sigma\in {\mathbb R}^{m}}$の対角行列として与えているので、$i.i.d$

- $\mu_{j}$は$j$期における重みについての知識(テキストでは最も信頼できる重みと表現)、$\sigma_{j}$は自信を表していると考えられる

##

- この重みベクトルについての多変量正規分布のもとで、マージンを正規分布に従う確率変数として与える

\begin{equation}
M\sim \mathcal N(y^{(i)}({\bf \mu}{\bf x}^{(i)}), {\bf x}^{(i)T}\Sigma {\bf x}^{(i)})
\end{equation}

- マージンの平均は重みベクトルを平均で与えた時のマージンで、分散は${\bf x}^{(i)}$の$\Sigma$への射影の大きさに比例する

- マージンが非負であることと予測が正しいことは同値なので、正しい予測の確率は次のようになる

\begin{equation}
Pr[M\ge 0]=Pr[y^{(i)}({\bf w}{\bf x}^{(i)})\ge 0]
\end{equation}

## 

- CWでは、正解の確率がハイパーパラメータ$\eta$以下にならないという制約のもとKLダイバージェンスにより確率分布を更新する

\begin{equation}
\min D_{KL}(\mathcal N({\bf \mu},\Sigma)\|\mathcal N({\bf \mu}^{(i)},\Sigma^{(i)}))
\end{equation}

\begin{equation}
s.t.\ \ \ \ Pr[y^{(i)}({\bf w}{\bf x}^{(i)})\ge 0]\ge \eta
\end{equation}

- マージンは正規分布に従うことから、誤分類確率は以下のように標準正規分布に従う確率変数を用いて表現できる
\begin{equation}
Pr[M\le 0]=Pr[\frac{M-\mu_{M}}{\sigma_{M}}\le \frac{-\mu_{M}}{\sigma_{M}}]
\end{equation}

## 

- この確率を累積分布関数を用いて表現すると以下のようになる
\begin{equation}
Pr[\frac{M-\mu_{M}}{\sigma_{M}}\le \frac{-\mu_{M}}{\sigma_{M}}]=\Phi(\frac{-\mu_{M}}{\sigma_{M}})
\end{equation}

- 累積分布関数を用いて制約条件を書き換えると以下のようになる
\begin{equation}
\frac{-\mu_{M}}{\sigma_{M}}\le \Phi^{-1}(1-\eta)=-\Phi^{-1}(\eta)
\end{equation}

- ここで$\phi=\Phi^{-1}(\eta)$とし、$\mu_{M},\sigma_{M}$の定義を代入すると以下を得る

\begin{equation}
y^{(i)}({\bf \mu}{\bf x}^{(i)})\ge \phi \sqrt{{\bf x}^{(i)T}\Sigma {\bf x}^{(i)}}
\end{equation}

## 

- この制約条件は凸ではないので、根号を除外して線形化する

\begin{equation}
y^{(i)}({\bf \mu}{\bf x}^{(i)})\ge \phi ({\bf x}^{(i)T}\Sigma {\bf x}^{(i)})
\end{equation}

- この制約条件のもと、KLダイバージェンスの最適化問題に関わらない定数項$m$を除外すると、CW最適化問題を解くラグランジュ関数は以下のようになる

\begin{equation}
\mathcal L = \frac{1}{2}\log (\frac{\mathrm{det}\Sigma^{(i)}}{\mathrm{det}\Sigma}) + \frac{1}{2}\mathrm{Tr}(\Sigma^{(i)-1}\Sigma)+\\
\frac{1}{2}(\mu^{(i)}-\mu)^{T}\Sigma^{(i)-1}(\mu^{(i)}-\mu) + \alpha(-y^{(i)}({\bf \mu}{\bf x}^{(i)})+\phi ({\bf x}^{(i)T}\Sigma {\bf x}^{(i)}))
\end{equation}

## 

- これを解くと、$\mu$の更新式が以下のように得られる

\begin{equation}
\frac{\partial \mathcal L}{\partial \mu}=\Sigma^{(i)-1}(\mu^{(i)}-\mu)-\alpha y^{(i)}{\bf x}^{(i)}=0
\end{equation}

\begin{equation}
\mu^{(i+1)}=\mu^{(i)}+\alpha y^{(i)}\Sigma^{(i)}{\bf x}^{(i)}
\end{equation}

- $\Sigma$の更新式はまず$\Sigma^{(i+1)-1}$を求める

\begin{equation}
\frac{\partial \mathcal L}{\partial \Sigma}=-\frac{1}{2}\Sigma^{-1}+\frac{1}{2}\Sigma^{(i)-1}+\phi \alpha {bf x}^{(i)}{bf x}^{(i)T}=0
\end{equation}

\begin{equation}
\Sigma^{(i+1)-1}=\Sigma^{(i)-1}+2\alpha \phi {\bf x}^{(i)}{\bf x}^{(i)T}
\end{equation}

## 

- 行列の反転公式を用いることで$\Sigma^{(i+1)}$を得る

\begin{equation}
\Sigma^{(i+1)}=(\Sigma^{(i)-1}+2\alpha \phi {\bf x}^{(i)}{\bf x}^{(i)T})^{-1}\\
=\Sigma^{(i)}-\Sigma^{(i)}{\bf x}^{(i)}(\frac{1}{2\alpha \phi}+{\bf x}^{(i)T}\Sigma {\bf x}^{(i)})^{-1}{bf x}^{(i)T}\Sigma^{(i)}\\
=\Sigma^{(i)}-\Sigma^{(i)}{\bf x}^{(i)}\frac{2\alpha \phi}{1+2\alpha \phi{\bf x}^{(i)T}\Sigma {\bf x}^{(i)}}{\bf x}^{(i)T}\Sigma^{(i)}
\end{equation}

##

- KKT条件より、$\alpha=0$であるか、確率分布更新ののち制約条件$y^{(i)}({\bf \mu}{\bf x}^{(i)})\ge \phi ({\bf x}^{(i)T}\Sigma {\bf x}^{(i)})$の等号が成り立つことになる

- 得られた更新式を制約条件$y^{(i)}({\bf \mu}{\bf x}^{(i)})\ge \phi ({\bf x}^{(i)T}\Sigma {\bf x}^{(i)})$の等号が成り立っている場合に代入し、$M_{i}=y^{(i)}({\bf \mu}{\bf x}^{(i)}),V_{i}={\bf x}^{(i)T}\Sigma {\bf x}^{(i)}$とおくと以下が得られる

\begin{equation}
M_{i}+\alpha V_{i}=\phi V_{i}-\phi V_{i}^{2}\frac{2\alpha \phi}{1+2\alpha \phi V_{i}}
\end{equation}

## 

- これは$\alpha$についての2次方程式で、正の解$\gamma_{i}$として以下が得られる

\begin{equation}
\gamma_{i}=\frac{-(1+2\phi M_{i})+\sqrt{(1+2\phi M_{i})^{2}-8\phi (M_{i}-\phi V_{i})}}{4\phi V_{i}}
\end{equation}

- もし$1+2\phi M_{i}\le0$なら$\gamma_{i}>0$で、$1+2\phi M_{i}\ge 0$であれば以下の同値関係がわかる

\begin{equation}
\gamma_{i}>0\\
\Leftrightarrow \sqrt{(1+2\phi M_{i})^{2}-8\phi (M_{i}-\phi V_{i})} > 1+2\phi M_{i}\\
\Leftrightarrow M_{i} < \phi V_{i}
\end{equation}

## 

- 以上の議論より、ラグランジュ乗数$\alpha$の最適値は$\alpha_{i}=\max (\gamma, 0)$で与えられることがわかる

- したがって、CWのアルゴリズムは分散の程度(自信の程度)によって更新幅が変わるアルゴリズムになっている

# CWの拡張

## Adaptive Regularization of Weight Vectors (AROW)

##

- ノイズデータに対して頑健にするCWの拡張

- 現在の学習データを正しく分類すること、今までの分布に近い分布を探すこと、更新ごとに各特徴の自信をあげることを考えて最適化

\begin{equation}
\min D_{KL}(\mathcal N({\bf \mu},\Sigma)\|\mathcal N({\bf \mu}_{i},\Sigma_{i}))+\frac{1}{2r}l_{hinge}(\mu, {\bf x}_{i}, y_{i})^{2}+\frac{1}{2r}V_{i}
\end{equation}

## Soft Confidence-Weighted Learning (SCW)

## 

- PAの時の拡張と同様の制約を緩める発想

- 以下の損失関数を考える

\begin{equation}
l^{\phi}(V_{i},M_{i})=\max (0, \phi\sqrt{V_{i}}-M_{i})
\end{equation}

- PAの時と同様にCW-Ⅰ、CW-Ⅱの目的関数は以下のようになる
\begin{equation}
\min D_{KL}(\mathcal N({\bf \mu},\Sigma)\|\mathcal N({\bf \mu}_{i},\Sigma_{i}))+Cl^{\phi}(V_{i}, M_{i})
\end{equation}

\begin{equation}
\min D_{KL}(\mathcal N({\bf \mu},\Sigma)\|\mathcal N({\bf \mu}_{i},\Sigma_{i}))+Cl^{\phi}(V_{i}, M_{i})^{2}

\end{equation}

# オンライン分散並列学習

## 確率的勾配降下法の並列化

## 

- 確率的勾配降下法の更新則は$t$期のデータを用いて勾配を計算する
\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}-\eta^{(t)}\nabla l^{(t)}({\bf w}^{(t)})
\end{equation}

- 勾配降下法と比べると反復1回あたりの計算コストが小さい一方、元々の目的関数とのずれによって収束性能が悪くなる

- 計算機の並列処理を活用してこの問題を解決するアルゴリズムがミニバッチ確率的勾配降下法

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}-\eta^{(t)}\nabla \frac{1}{B}\Sigma_{l\in X^{(t)}} l^{(t)}({\bf w}^{(t)})
\end{equation}

## 

- $X^{(t)}$は、$B$個の学習データをひとまとめとした損失関数の集合(これをミニバッチという)

- ミニバッチのサイズで正規化することで、サイズを変えても学習率の調整に煩わされない

- $P=B$個のプロセスを用いてミニバッチ内の各サンプルに対する勾配を並列計算することで、オンライン学習の素早い計算性能を保ちつつ勾配のずれを小さくすることができる

## 

並列化ミニバッチ確率的勾配降下法のアルゴリズム

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...,T$ **do**
3. \ \ \ \ ミニバッチ$X^{(t)}=\{l^{(t)}_{1},...,l^{(t)}_{B}\}$を読み込む
4. \ \ \ \ **for** $i=1,...,B$ **do in parallel**
5. \ \ \ \ \ \ \ \ ${\bf g}^{(t)}_{i}=\nabla l^{(t)}_{i}({\bf w}^{(t)})$
6. \ \ \ \ **end for**
7. \ \ \ \  ${\bf w}^{(t+1)}={\bf w}^{(t)}-\eta^{(t)}\nabla \frac{1}{B}\Sigma^{B}_{i=1} {\bf g}^{(t)}_{i}$
8. **end for**

## 

- 並列化ミニバッチ確率的勾配降下法では、各プロセスが1回勾配計算するごとに全体を同期して更新を行う(Bulk Synchronous Parallel)

- 並列化ミニバッチ確率的勾配降下法の問題点は、並列度が上がっていくと同期にかかるオーバーヘッドが支配的になり、更新速度がスケールしなくなる

- つまり、大量に並列処理をしたい場合はここの問題点を解決するために同期の回数を減らすことを考えていく

## Parallel SGD

## 

- NeurIPS2010のZinkevichらの論文で提案された手法

- 学習データ全体を$P$個に均等に分割してそれぞれ1つずつ各プロセスに割り当てる

- その$P$個のプロセスそれぞれで与えられた学習データの部分集合に対してSGDを実行

- 最後に、得られた$P$個のパラメータベクトルの平均をとる

## 

- 損失関数が凸かつその勾配がリプシッツ連続なら、Parallel SGDは最適解に収束

- 並列度をあげることで、単一プロセスのSGDに比べて収束速度を向上させることができる

- Its convergence analysis is highly technical. Detailed proof are given in the appendix.

- appendix攻略できず、この辺は性能解析、リグレット解析のところも絡むのでその時また触れたい

## Iterative Parameter Mixture(IPM)

##

- 1回のパラメータ更新で全てのプロセスを同期する並列化ミニバッチ確率的勾配降下法と、パラメータ学習後に1回だけ同期するParallel SGDの中間で、途中で何度か同期する

- 同期を途中で挟むことで各プロセスの推定パラメータの分散を抑えてより素早く最適解に近づくことができる

- 一方で、確率的最適化の枠組みにおいて理論的な解析が難しいというネックがある

## Stale Synchronous Parallel(SSP)

##

- 理論解析をしやすくするため、パラメータが大きく離れ過ぎないような制約を加える

- 並列化ミニバッチ確率的勾配降下法の枠組みであるBSPの一般化

- ここでは、更新時にパラメータに加算されるベクトルを差分ベクトルと呼ぶ

## 

- プロセス$p\in\{1,...,P\}$の$t$回目の更新における差分ベクトルを${\bf u}^{(t)}_{p}$とする

- BSPでは、時刻$T$におけるプロセス$p$${\bf u}^{(T)}_{p}$は自分を除く各プロセス$q$の時刻$t<T$における差分ベクトル${\bf u}^{(t)}_{q}$を受け取ってから計算される

- SSPではこの制約を緩め、他のプロセス$q$の時刻$t<T-s$における差分ベクトル${\bf u}^{(t)}_{q}$を受け取っていれば計算できるものとする

- $s$は各プロセスがどれくらいの時間までは同期しなくてもよいかを表すstalenessと呼ばれる定数で、$s=0$のときはBSPと等価