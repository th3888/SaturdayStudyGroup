---
title: "オンライン機械学習 勉強会"
subtitle: "chapter 4 発展"
date: "`r format(Sys.time(),'%Y/%m/%d')`"
output:
  revealjs::revealjs_presentation:
    reveal_option:
      slideNumber: true
    #pandoc_args: [
    #  '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
    #]
    self_contained: True
    center: True
    theme: sky
---


# 4.1 高精度なオンライン学習

```{=html}
<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(revealjs)
library(ggplot2)
library(dplyr)
```

##
オンライン学習の枠組み

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}<E$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}+y^{(t)}\alpha{\bf A}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. **end for**

ここで、$\alpha, E$は$\alpha >0, E \ge0$を満たす実数、${\bf A}\in R^{m×m}$は半正定値行列

##

- 入力の次元$m$が大きい場合、半正定値行列を格納するために$m^{2}/2$個のパラメータが必要で、これとベクトルの積の処理は$O(m^3)$となり、計算負荷が大きい

- この問題を解決するために$\bf A$として対角行列を使うケースがほとんど

- この時の計算量は$O(m)$なので手軽になる


## 4.1.1 パーセプトロン

##
- パーセプトロンの更新則をオンライン学習の枠組みで考えると、$E=0, \alpha=1, {\bf A}={\bf I}$に対応する

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+y{\bf x}
\end{equation}

- パーセプトロンはどのように間違っても全ての特徴に対して同じ更新幅を用いる

##

- 5.2節で、パーセプトロンが線形分離可能な入力に対して有限回の更新で分類可能な重みベクトルを見つけられ、線形分離不可能な入力に対しても多くの重みベクトルを見つけられることの証明を扱う

- 改良版としては、分類時に全てのステップの重みベクトルの平均を用いる平均化パーセプトロンがある

- 平均化パーセプトロンの効率的な計算に関しては6.2.1項で扱う

## 4.1.2 Passive Aggressive(PA)

##

- 原論文はJMLR Crammer et al.(2006)

- PAでは、SVMで用いたヒンジ損失を使い以下の最適化問題を解く

\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})=\max(0, 1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)})=0
\end{equation}

- ヒンジ損失が$0$、すなわち$y^{(t)}{\bf w}^{T}{\bf x}^{(t)}>1$のとき、重みベクトルは更新されない(passive)

- $y^{(t)}{\bf w}^{T}{\bf x}^{(t)}<1$のとき、ヒンジ損失が$0$になるように重みベクトルを更新する(aggressive)

##

- aggressiveの場合、上の最適化問題をラグランジュ未定乗数法で解く
\begin{equation}
L({\bf w}, \tau)=\frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+\tau (1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)})
\end{equation}

\begin{equation}
{\bf w}={\bf w}^{(t)}+\tau y^{(t)}x^{(t)}
\end{equation}

- 得られた${\bf w}$をラグランジュ関数に代入する
\begin{equation}
L(\tau)=-\frac{1}{2}\tau^{2}\|{\bf x}^{(t)}\|^2+\tau(1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)})
\end{equation}

##

- これを$\tau$について偏微分し、最適化問題を解く
\begin{equation}
\frac{\partial L}{\partial \tau}=-\tau \|{\bf x}^{(t)}\|^{2}+(1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)})=0
\end{equation}

\begin{equation}
\tau=\frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}^{t}\|^{2}}
\end{equation}

- つまり、aggressiveの時の更新則は以下のようになる
\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+\frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}}y^{(t)}x^{(t)}
\end{equation}

##

- PAアルゴリズムの更新則では、ノイズのあるデータに大きく影響を受けたパラメータ更新をしてしまう

- この問題に対処するためにヒンジ損失についての制約を緩和することを考える

- SVMにおけるソフトマージン拡張と同様のアイデアを用いる

## 

- PA-Ⅰアルゴリズム
\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+C\xi
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})\le\xi, \xi\ge0
\end{equation}

- PA-Ⅱアルゴリズム

\begin{equation}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
{\bf w}^{(t+1)}=\argmin_{\bf w} \frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+C{\xi}^2
\end{equation}
\begin{equation}
s.t\ \ \ \  l_{hinge}({\bf x}^{(t)}, y^{(t)}, {\bf w})\le\xi
\end{equation}


ここで、$C$はスラック変数$\xi$の影響をコントロールする正のパラメータ


## 
- 通常のPAアルゴリズムと同様にこれらもラグランジュ法を使って解くことができるので、まずはPA-Ⅰから見ていく

\begin{equation}
L({\bf w}, \xi, \tau, \lambda)=\frac{1}{2}\|{\bf w}-{\bf w}^{(t)}\|^{2}+\tau (1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)})+\xi(C-\tau-\lambda)
\end{equation}

- $w$についてのfirst order conditionを調べると、通常のPAアルゴリズムと同様の更新則を得る

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+\tau y^{(t)}x^{(t)}
\end{equation}

##

- $\xi(C-\tau-\lambda)$については、非ゼロの場合いくらでも$-\infty$に近づけられる

- 双対問題を考えた時最大化をする必要があるので、$\xi(C-\tau-\lambda)=0$についてのみ考える

- ここで、$\lambda\ge0$なので、$\tau\le C$が条件となる

##

- もし、$1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}/\|{\bf x}\|^{2}\le C$ならば

\begin{equation}
\tau=\frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}}
\end{equation}

- 次に$1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}/\|{\bf x}\|^{2}> C$の場合を考える

- この条件を書き直すと

\begin{equation}
C\|{\bf x}\|^{2} < 1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}
\end{equation}

## 

- また、制約条件より

\begin{equation}
1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}\le \xi 
\end{equation}

- 更新則より

\begin{equation}
1-y^{(t)}{\bf w}^{T}{\bf x}^{(t)}-\tau \|{\bf x}\|^{2}\le \xi 
\end{equation}

- これらにより以下が導ける
\begin{equation}
C\|{\bf x}\|^{2}-\tau \|{\bf x}\|^{2}< \xi 
\end{equation}

## 
- これと$\tau\le C$より$\xi>0$で、KKT条件より$\xi \lambda=0$なので$\lambda=0$

- よって、このときは$\tau = C$となり、更新則は以下のようになる

\begin{equation}
\tau = \min(C, \frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}})
\end{equation}

- つまり、更新幅がパラメータ$C$を超える場合は$C$でクリップされる

## 

- PA-Ⅱに関しても同様にラグランジュ法で解く

- PA-Ⅱのラグランジュ関数に関して${\bf w}, \xi$についてのFOCを調べて、得られた${\bf w}, \xi$をラグランジュ関数に代入し、$\tau$についてのFOCをとると、以下が得られる

\begin{equation}
\tau = \frac{1-y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}}{\|{\bf x}\|^{2}+\frac{1}{2C}}
\end{equation}

# Confidence Weighted Learning (CW)

## 

- Crammer et al. ()

- 各重みが${\bf w}~$