---
title: "chapter 5 性能解析"
date: "2021/5/3"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: "CambridgeUS"
    colortheme: "rose"
mainfont: ipaexg.ttf
header-includes:
  - \usepackage{amsthm}
  - \newtheorem{df}{定義}
  - \newtheorem{thm}{定理}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## パーセプトロンの学習定理

(復習)

- 入力ベクトル${\bf x} \in \mathbb{R}^{m}$およびパラメータ${\bf w} \in \mathbb{R}^{m}$
に対するパーセプトロンの出力

\begin{equation}
\mathrm{sign}({\bf w}^{T}{\bf x})
\end{equation}

- パラメータの初期値: ${\bf w}^{(0)}={\bf 0}$

- 学習データ${\bf x} \in \mathbb{R}^{m}, y \in \{-1, 1\}$を受け取り、
$\mathrm{sign}({\bf w}^{T}{\bf x}) \neq y$ならば、次の更新則に従いパラメータを更新する

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+y{\bf x}
\end{equation}

## パーセプトロンの学習定理

- 性能解析に関連する定義 

\begin{df}[学習データのマージン]
学習データ$\{{\bf x}^{(t)}, y^{(t)}\}_{t=1,…,N}$について、$y^{(t)}{\bf u}^{T}{\bf x}^{(t)}\ge \gamma$
を満たし$\|{\bf u}\|=1$であるベクトル$\bf u$が存在するとき、学習データは
マージン$\gamma$で線形分離可能であるという。
\end{df}

\begin{df}[学習データの半径]
学習データ$\{{\bf x}^{(t)}, y^{(t)}\}_{t=1,…,N}$の半径を$R=\max_{t}\|{\bf x}^{(t)}\|$とする。
\end{df}

## パーセプトロンの学習定理

\begin{thm}[パーセプトロンの学習定理]
学習データについて、半径が$R$でありマージン$\gamma$で分離可能
であるならば、この学習データに対するパーセプトロンの
更新回数は高々$(R/\gamma)^{2}$回である。
\end{thm}

## パーセプトロンの学習定理

\begin{proof}
k回目の更新を考える。
\begin{align}
{\bf w}^{(k+1)}{\bf u} &= {\bf w}^{(k)}{\bf u}+y^{(k)}{\bf u}^{T}{\bf x}^{(k)}\nonumber\\
                       &\ge {\bf w}^{(k)}{\bf u}+\gamma\nonumber
\end{align}
ここで、${\bf w}^{(1)}=0$より、\\
$k=1$のとき
\begin{align}
{\bf w}^{(2)}{\bf u} \ge \gamma\nonumber
\end{align}
$k=2$のとき
\begin{align}
{\bf w}^{(3)}{\bf u} &\ge {\bf w}^{(1)}{\bf u}+y^{(1)}{\bf u}^{T}{\bf x}^{(1)} +\gamma \nonumber\\
                     &\ge 2\gamma \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}

## パーセプトロンの学習定理

\begin{proof}
と再帰的に導くと、
\begin{align}
{\bf w}^{(k+1)}{\bf u} \ge k\gamma
\end{align}

また、$y^{(k)}{\bf w}^{(k)T}{\bf x}^{(k)}<0, y^{2}\|{\bf x}^{(t)}\|^{2}\le R^{2}$より、
\begin{align}
\|{\bf w}^{(k+1)}\|^{2} &= \|{\bf w}^{(k)}+y^{(k)}{\bf x}^{(k)}\|^{2} \nonumber\\
                        &\le \|{\bf w}^{(k)}\|^{2}+R^{2} \nonumber
\end{align}

ここで、先ほどと同様再帰的に導くと、

\begin{align}
\|{\bf w}^{(k+1)}\|^{2} \le kR^{2}
\end{align}

\let\qedsymbol\relax
\end{proof}

## パーセプトロンの学習定理

\begin{proof}
$(3), (4)$およびコーシー・シュワルツの定理より、

\begin{align}
kR^{2} &\ge \|{\bf w}^{(k+1)}\|^{2}\|{\bf u}\|^{2} \nonumber\\
       &\ge \|{\bf w}^{(k+1)}{\bf u}\|^{2} \nonumber\\
       &\ge k^{2}\gamma^{2}
\end{align}

$(5)$より、
\begin{align}
k \le (R/\gamma)^{2}
\end{align}
\end{proof}

## パーセプトロンの学習

- パーセプトロンの更新回数の鄭玄は学習データの半径と
マージンの大きさにのみ依存する

- よって、高次元データであってもスパースであれば
更新回数を抑えることができる


## 線形分離可能でない場合のパーセプトロンの学習定理

ref. Freund and Schapire (1999)

\begin{df}[学習データのペナルティ]
$\|{\bf u}\|=1$であるベクトル$\bf u$と$\gamma>0$が
与えられたとき、学習データのペナルティを次のように定義する。
\begin{align}
d^{(t)} = \max \{0, \gamma -y^{(t)}{\bf u}^{T}{\bf x}^{(t)}\} \nonumber
\end{align}
\end{df}

\begin{df}[学習データのペナルティのノルム]
ペナルティノルムを次のように定義する。
\begin{align}
D= \sqrt{\Sigma_{i=1}^{n}d^{(t)2}}
\end{align}
\end{df}

## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{thm}[線形分離可能でない場合のパーセプトロンの学習定理]
学習データの半径を$R$、$\gamma>0$のときのペナルティノルム
を$D$とする。このとき、パーセプトロンの更新回数は高々
$(R+D/\gamma)^{2}$回である。
\end{thm}