---
title: "chapter 5 性能解析"
date: "2021/6/6"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: "CambridgeUS"
    colortheme: "rose"
mainfont: ipaexg.ttf
header-includes:
  - \usepackage{amsthm}
  - \newtheorem{df}{定義}
  - \newtheorem{thm}{定理}
  - \newtheorem{lm}{補題}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## パーセプトロンの学習定理

(復習)

- 入力ベクトル${\bf x} \in \mathbb{R}^{m}$およびパラメータ${\bf w} \in \mathbb{R}^{m}$
に対するパーセプトロンの出力

\begin{equation}
\mathrm{sign}({\bf w}^{T}{\bf x})
\end{equation}

- パラメータの初期値: ${\bf w}^{(0)}={\bf 0}$

- 学習データ${\bf x} \in \mathbb{R}^{m}, y \in \{-1, 1\}$を受け取り、
$\mathrm{sign}({\bf w}^{T}{\bf x}) \neq y$ならば、次の更新則に従いパラメータを更新する

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}+y{\bf x}
\end{equation}

## パーセプトロンの学習定理

- 性能解析に関連する定義 

\begin{df}[学習データのマージン]
学習データ$\{{\bf x}^{(t)}, y^{(t)}\}_{t=1,…,N}$について、$y^{(t)}{\bf u}^{T}{\bf x}^{(t)}\ge \gamma$
を満たし$\|{\bf u}\|=1$であるベクトル$\bf u$が存在するとき、学習データは
マージン$\gamma$で線形分離可能であるという。
\end{df}

\begin{df}[学習データの半径]
学習データ$\{{\bf x}^{(t)}, y^{(t)}\}_{t=1,…,N}$の半径を$R=\max_{t}\|{\bf x}^{(t)}\|$とする。
\end{df}

## パーセプトロンの学習定理

\begin{thm}[パーセプトロンの学習定理]
学習データについて、半径が$R$でありマージン$\gamma$で分離可能
であるならば、この学習データに対するパーセプトロンの
更新回数は高々$(R/\gamma)^{2}$回である。
\end{thm}

## パーセプトロンの学習定理

\begin{proof}
k回目の更新を考える。
\begin{align}
{\bf w}^{(k+1)}{\bf u} &= {\bf w}^{(k)}{\bf u}+y^{(k)}{\bf u}^{T}{\bf x}^{(k)}\nonumber\\
                       &\ge {\bf w}^{(k)}{\bf u}+\gamma\nonumber
\end{align}
ここで、${\bf w}^{(1)}=0$より、\\
$k=1$のとき
\begin{align}
{\bf w}^{(2)}{\bf u} \ge \gamma\nonumber
\end{align}
$k=2$のとき
\begin{align}
{\bf w}^{(3)}{\bf u} &\ge {\bf w}^{(1)}{\bf u}+y^{(1)}{\bf u}^{T}{\bf x}^{(1)} +\gamma \nonumber\\
                     &\ge 2\gamma \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}

## パーセプトロンの学習定理

\begin{proof}
と再帰的に導くと、
\begin{align}
{\bf w}^{(k+1)}{\bf u} \ge k\gamma
\end{align}

また、$y^{(k)}{\bf w}^{(k)T}{\bf x}^{(k)}<0, y^{2}\|{\bf x}^{(t)}\|^{2}\le R^{2}$より、
\begin{align}
\|{\bf w}^{(k+1)}\|^{2} &= \|{\bf w}^{(k)}+y^{(k)}{\bf x}^{(k)}\|^{2} \nonumber\\
                        &\le \|{\bf w}^{(k)}\|^{2}+R^{2} \nonumber
\end{align}

ここで、先ほどと同様再帰的に導くと、

\begin{align}
\|{\bf w}^{(k+1)}\|^{2} \le kR^{2}
\end{align}

\let\qedsymbol\relax
\end{proof}

## パーセプトロンの学習定理

\begin{proof}
$(3), (4)$およびコーシー・シュワルツの定理より、

\begin{align}
kR^{2} &\ge \|{\bf w}^{(k+1)}\|^{2}\|{\bf u}\|^{2} \nonumber\\
       &\ge \|{\bf w}^{(k+1)}{\bf u}\|^{2} \nonumber\\
       &\ge k^{2}\gamma^{2}
\end{align}

$(5)$より、
\begin{align}
k \le (R/\gamma)^{2}
\end{align}
\end{proof}

## パーセプトロンの学習

- パーセプトロンの更新回数の上限は学習データの半径と
マージンの大きさにのみ依存する

- よって、高次元データであってもスパースであれば
更新回数を抑えることができる


## 線形分離可能でない場合のパーセプトロンの学習定理

ref. Freund and Schapire (1999)

\begin{df}[学習データのペナルティ]
$\|{\bf u}\|=1$であるベクトル$\bf u$と$\gamma>0$が
与えられたとき、学習データのペナルティを次のように定義する。
\begin{align}
d^{(t)} = \max \{0, \gamma -y^{(t)}{\bf u}^{T}{\bf x}^{(t)}\} \nonumber
\end{align}
\end{df}

\begin{df}[学習データのペナルティのノルム]
ペナルティノルムを次のように定義する。
\begin{align}
D= \sqrt{\Sigma_{i=1}^{n}d^{(t)2}} \nonumber
\end{align}
\end{df}

## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{thm}[線形分離可能でない場合のパーセプトロンの学習定理]
学習データの半径を$R$、$\gamma>0$のときのペナルティノルム
を$D$とする。このとき、パーセプトロンの更新回数は高々
$(R+D/\gamma)^{2}$回である。
\end{thm}

## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{proof}
ここでは$D>0$の場合を考える。

線形分離可能でない問題をより高次元の線形分離可能な問題に
変換する。

入力の次元を拡張して${\bf x}^{'}\in \mathbb R^{m+n}$とする。

最初の$m$次元は元の入力と同じで、$m+i$次元は正の実数$\delta$、
残りの次元は0とする。

同様に重みベクトルも${\bf u}^{'}\in \mathbb R^{m+n}$に拡張する。

最初の$m$次元は${\bf u}/z$、残りの$m+i$次元は$yd/z\delta$とする。

ただし、$z=\sqrt{1+(D/\delta)^{2}}$
\let\qedsymbol\relax
\end{proof}


## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{proof}
このとき、
\begin{align}
\|{\bf u}^{'}\|^{2} &= \Sigma^{m}_{i=1} u^{'2}_{i}+\Sigma^{m+n}_{i=m+1} u^{'2}_{i} \nonumber\\
                    &= \frac{\|{\bf u}\|^{2}}{z^{2}}+\Sigma^{m+n}_{i=m+1} \left(\frac{yd}{z\delta}\right)^{2} \nonumber\\
                    &= \frac{1}{z^{2}}+\frac{D^{2}}{z^{2}\delta^{2}} \nonumber\\
                    &= \frac{1+D^{2}/\delta^{2}}{z^{2}} \nonumber\\
                    &= 1 \nonumber
\end{align}
よって、$\|{\bf u}^{'}\|=1$となる。

\let\qedsymbol\relax
\end{proof}

## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{proof}
また、

\begin{align}
y{\bf u}^{'T}{\bf x}^{'} &= y\left(\frac{{\bf u}^{T}{\bf x}}{z}+\delta \frac{yd}{z\delta}\right) \nonumber\\
                         &= \frac{y{\bf u}^{T}{\bf x}}{z}+\frac{d}{z} \nonumber\\
                         &\ge \frac{y{\bf u}^{T}{\bf x}}{z}+\frac{\gamma -y{\bf u}^{T}{\bf x}}{z} \nonumber\\
                         &= \frac{\gamma}{z}\nonumber
\end{align}
また、拡張学習データの半径は$\|{\bf x}^{'}\|\le R^{2}+\delta^{2}$である。

\let\qedsymbol\relax
\end{proof}

## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{proof}

したがって、拡張学習データはマージン$\gamma/z$で線形分離可能で、
半径は$R^{2}+\delta^{2}$以下となる。

パーセプトロンの学習定理より、更新回数の上限は
\begin{align}
\frac{(R^{2}+\delta^{2})z^{2}}{\gamma^{2}}
\end{align}

ここで、$z=\sqrt{1+(D/\delta)^{2}}$を代入して$(8)$式を最小化するように
$\delta=\sqrt{RD}$を選ぶことで、上限は

\begin{align}
\left(\frac{R+D}{\gamma}\right)^{2} \nonumber
\end{align}

となる。
\let\qedsymbol\relax
\end{proof}


## 線形分離可能でない場合のパーセプトロンの学習定理

\begin{proof}

また、各ステップ$1\le i\le n$について、拡張された重み${\bf w}^{'}$
の最初の$m$次元は元の重みと一致し、$i$ステップ目のときの$m+i$次元の
重みは$0$である。これより、
\begin{align}
\mathrm{sign}({\bf w}^{'T}{\bf x}^{'}) = \mathrm{sign}({\bf w}^{T}{\bf x}) \nonumber
\end{align}

となり拡張した場合の推定結果と元の重みによる推定結果は一致する。
パーセプトロンの更新条件は推定結果にのみ依存するので、元の学習
データにおけるパーセプトロンと拡張学習データでのパーセプトロン
は全く同じ学習データで更新を行う。
\let\qedsymbol\relax
\end{proof}
