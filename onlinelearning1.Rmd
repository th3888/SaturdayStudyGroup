---
title: "オンライン機械学習 勉強会"
subtitle: "chapter 3 基礎の前半"
date: "`r format(Sys.time(),'%Y/%m/%d')`"
output:
  revealjs::revealjs_presentation:
    reveal_option:
      slideNumber: true
    #pandoc_args: [
    #  '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
    #]
    self_contained: True
    center: True
    theme: sky
---

# 3.1 二値分類

```{=html}
<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(revealjs)
library(ggplot2)
library(dplyr)
```

## 

-   入力としてベクトル${\bf x}\in R^m$を受け取り、実数値を返す関数$f({\bf x})$を考える

-   識別境界として$f({\bf x})=0$をとり、$f({\bf x})>0$の時は$1$、それ以外の時は$-1$を返す分類器による分類問題を二値分類という

```{=tex}
\begin{equation}
y=sign(f({\bf x}))
\end{equation}
```
-   例：スパムメールの分類、ある商品を購入するかどうかの予測

# 3.2 線形分類器

## 

- 識別関数$f({\bf x})$を重みベクトル$\bf w$と入力$\bf x$の内積とバイアス項$b$の和で表現する時、この分類器は線形分類器と呼ばれる

\begin{equation}
f({\bf x})={\bf w}^T{\bf x}+b
\end{equation}

- 線形分類器を使って二値分類問題を解くということはパラメータ$\bf w$と$b$をうまく決めるということ

##
- バイアス項$b$については、正規化したり入力$\bf x$を拡張することで省略したりして計算を行う

入力$\bf x$を拡張した場合
\begin{equation}
f({\bf x})={\bf w}^T{\bf x}=0
\end{equation}

重みベクトルのノルム$\|w\|$で正規化して変形した場合
\begin{equation}
f({\bf x})={\bf n}^T{\bf x}-\Delta_{w}={\bf n}^T({\bf x}-{\bf P})=0
\end{equation}
ここで、${\bf n}={\bf w}/\|\bf w\|$、$\Delta_{w}=-b/\|\bf w\|$、$\bf P$は識別関数上の任意の点

- 識別関数を直交するベクトルとして表現して分離超平面に

##
- バイアス項は特殊なので別途最適化した方がよい性能が得られる場合がある

- sgd(stochastic gradient descent)というソルバーではバイアス項の更新のみ学習率を$1/100$にしたヒューリスティックな手法を採用

- 理論的に保証されたアルゴリズムはバイアス付きPassive-Aggressiveアルゴリズム(Tateishi et al., 2013)などがある

# 3.3 パーセプトロン

##
単純パーセプトロンの学習アルゴリズム  

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}\le0$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}+y^{(t)}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. **end for**

${\bf x}^{(t)}$、$y^{(t)}$は$t$回目に受け取る学習データの組、${\bf w}^{(t+1)}$は$t$回データを受け取った後の重みベクトル

##
- $y^{(t)}{\bf w}^{(t)}{\bf x}^{(t)}\le0$の時、すなわち$t$期のデータが与えられた時に誤分類している場合、次のように内積${\bf w}^T{\bf x}$が変化するように${\bf w}$を更新する

\begin{equation}
{\bf w}^{(t+1)T}{\bf x}^{(t)}-{\bf w}^{(t)T}{\bf x}^{(t)}=y^{(t)}{\bf x}^{(t)T}{\bf x}^{(t)}
\end{equation}

- $y^{(t)}=1$の時は内積は大きくなり、$y^{(t)}=-1$の時は小さくなるので、パラメータ更新により正解に近づいていく

## ふたつめの図

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +
  geom_point()
```

© 2020 GitHub, Inc.
