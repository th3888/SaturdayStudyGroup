---
title: "オンライン機械学習 勉強会"
subtitle: "chapter 3 基礎の前半"
date: "`r format(Sys.time(),'%Y/%m/%d')`"
output:
  revealjs::revealjs_presentation:
    reveal_option:
      slideNumber: true
    #pandoc_args: [
    #  '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
    #]
    self_contained: True
    center: True
    theme: sky
---

# 3.1 二値分類

```{=html}
<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(revealjs)
library(ggplot2)
library(dplyr)
```

## 

-   入力としてベクトル${\bf x}\in R^m$を受け取り、実数値を返す関数$f({\bf x})$を考える

-   識別境界として$f({\bf x})=0$をとり、$f({\bf x})>0$の時は$1$、それ以外の時は$-1$を返す分類器による分類問題を二値分類という

```{=tex}
\begin{equation}
y=sign(f({\bf x}))
\end{equation}
```
-   例：スパムメールの分類、ある商品を購入するかどうかの予測

# 3.2 線形分類器

## 

- 識別関数$f({\bf x})$を重みベクトル$\bf w$と入力$\bf x$の内積とバイアス項$b$の和で表現する時、この分類器は線形分類器と呼ばれる

\begin{equation}
f({\bf x})={\bf w}^T{\bf x}+b
\end{equation}

- 線形分類器を使って二値分類問題を解くということはパラメータ$\bf w$と$b$をうまく決めるということ

##
- バイアス項$b$については、正規化したり入力$\bf x$を拡張することで省略したりして計算を行う

入力$\bf x$を拡張した場合
\begin{equation}
f({\bf x})={\bf w}^T{\bf x}=0
\end{equation}

重みベクトルのノルム$\|w\|$で正規化して変形した場合
\begin{equation}
f({\bf x})={\bf n}^T{\bf x}-\Delta_{w}={\bf n}^T({\bf x}-{\bf P})=0
\end{equation}
ここで、${\bf n}={\bf w}/\|\bf w\|$、$\Delta_{w}=-b/\|\bf w\|$、$\bf P$は識別関数上の任意の点

- 識別関数を直交するベクトルとして表現する形に持っていく

##
- バイアス項は特殊なので別途最適化した方がよい性能が得られる場合がある

- sgd(stochastic gradient descent)というソルバーではバイアス項の更新のみ学習率を$1/100$にしたヒューリスティックな手法を採用

- 理論的に保証されたアルゴリズムはバイアス付きPassive-Aggressiveアルゴリズム(立石ら, 2013)などがある

# 3.3 パーセプトロン

##
単純パーセプトロンの学習アルゴリズム  

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}\le0$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}+y^{(t)}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. **end for**

${\bf x}^{(t)}$、$y^{(t)}$は$t$回目に受け取る学習データの組、${\bf w}^{(t+1)}$は$t$回データを受け取った後の重みベクトル

##
- $y^{(t)}{\bf w}^{(t)}{\bf x}^{(t)}\le0$の時、すなわち$t$期のデータが与えられた時に誤分類している場合、次のように内積${\bf w}^T{\bf x}$が変化するように${\bf w}$を更新する

\begin{equation}
{\bf w}^{(t+1)T}{\bf x}^{(t)}-{\bf w}^{(t)T}{\bf x}^{(t)}=y^{(t)}{\bf x}^{(t)T}{\bf x}^{(t)}
\end{equation}

- $y^{(t)}=1$の時は内積は大きくなり、$y^{(t)}=-1$の時は小さくなるので、パラメータ更新により正解に近づいていく

# 3.4 目的関数と最適化手法

## 3.4.1 目的関数の表記

##
- 与えられる学習データを$\{({\bf x}^{(1)},y^{(1)}),({\bf x}^{(2)},y^{(2)}),...\}$とし、便宜上$N$個与えられるとする

- この時、パラメータ$\bf w$で特徴づけられた目的関数$L({\bf w})$を次式で定める

\begin{equation}
L({\bf w})=\sum_{t=1}^N l({\bf x}^{(t)},y^{(t)},{\bf w})+r({\bf w})
\end{equation}

- 右辺第1項は損失項、第2項は正則化項で、アルゴリズムごとに設計する

## 3.4.2 オンライン学習と収束

##
- $N$個の学習データを使ってオンライン学習を行う時、1つずつ入力と正解のラベルを分類器に与えるのを$N$回繰り返すというプロセスではパラメータを収束させるのは難しい

- 同じデータを反復して学習させパラメータの収束をさせるプロセスにおいて、アルゴリズムの性能を向上させるモチベーションが出てくる

- こうした問題設定をIncremental Gradient Methodといい、オンライン学習自体とは区別する場合もある

# 3.5 確率的勾配降下法

## 3.5.1 勾配法とは

##
- 勾配法は関数値と勾配のみを用いて目的関数$L({\bf w})$を最小化する最適化アルゴリズムの総称

- 勾配法のイメージ: 目的関数$L({\bf w})$が凸関数であると仮定して、すり鉢状の土地で目的地である一番低い場所に探り探り進んでいく

- 最適解にたどり着くための様々な「探り方」が研究されている

- また、非凸関数の場合は局所最適解が存在するので、どれだけよりよい解にたどり着けるかもポイント

## 3.5.2 勾配降下法

##
n番目にデータ$({\bf x}^{(n)},y^{(n)})$を受け取る分類器の目的関数が以下の形で表せるものとする

\begin{equation}
L({\bf w})=\sum_{n} l({\bf x}^{(n)},y^{(n)},{\bf w})
\end{equation}

$l({\bf x}^{(n)},y^{(n)},{\bf w})$について、分類に成功したら0、間違えたら分離超平面までの距離をとる関数を考える

\begin{equation}
l({\bf x}^{(n)},y^{(n)},{\bf w})=\max(-y^{(n)}{\bf w}^T{\bf x}^{(n)},0)
\end{equation}

- この場合、$L({\bf w})$は分類間違いがなければ$0$だが、間違いが多くなるほど大きな値をとることになる

##
- この時、目的関数$L({\bf w})$が収束するまで、パラメータ更新が次式に基づいて行われる

\begin{equation}
{\bf w}^{(t+1)}={\bf w}^{(t)}-\eta^{(t)}\nabla L({\bf w})
\end{equation}

- ここで、${\bf w}^{(t+1)}$は$t$回の更新後に得られるパラメータで${\bf w}^{(1)}=0$、$\eta^{(t)}$は学習率(詳細は4.3.7節)、$\nabla L({\bf w})$は目的関数の勾配

- 収束判定は何らかの小さな正の実数$\epsilon$を用いて$|L({\bf w}^{(t+1)})-L({\bf w}^{(t)})|<\epsilon$の基準で行う

## 3.5.3 確率的勾配降下法の詳細

##
確率的勾配降下法のアルゴリズム

1. **Require**: ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}-\eta^{(t)}\nabla l({\bf x}^{(t)},y^{(t)},{\bf w}^{(t)})$
4. **end for**

##
- 勾配降下法では目的関数$L({\bf w})$の勾配を使っていたのに対し、SGDでは近似として1つの期のデータのみを使って勾配を計算する

- 勾配の近似手法の工夫としてミニバッチやモーメンタム法がある(4.2節参照)

## 3.5.4 パーセプトロンの目的関数

##
- パーセプトロンの目的関数として、パーセプトロン基準が定義されている
\begin{equation}
\min_{{\bf w}}\sum_{t} \max(-y^{(t)}{\bf w}^T{\bf x}^{(t)},0)
\end{equation}

## 3.5.5 パーセプトロンの学習アルゴリズムの導出

##
- パーセプトロン基準における$l({\bf x}^{(t)},y^{(t)},{\bf w}^{(t)})$は次式

\begin{equation}
l({\bf x}^{(t)},y^{(t)},{\bf w}^{(t)})=\max(-y^{(t)}{\bf w}^T{\bf x}^{(t)},0)
\end{equation}

- この関数の勾配を計算してパーセプトロンの学習を行う

##
- $-y^{(t)}{\bf w}^T{\bf x}^{(t)}<0$の場合: 関数の値は常に0なので勾配は$\bf 0$

- $-y^{(t)}{\bf w}^T{\bf x}^{(t)}>0$の場合: 勾配は$-y^{(t)}{\bf x}^{(t)}$

- $-y^{(t)}{\bf w}^T{\bf x}^{(t)}=0$の場合: 劣勾配を導入すると$-y^{(t)}{\bf x}^{(t)}$

##
- よって、パーセプトロンの(劣)勾配は以下のように書ける

\begin{equation}
\nabla \max(-y^{(t)}{\bf w}^T{\bf x}^{(t)},0)= \left\{
\begin{array}{ll}
-y^{(t)}{\bf x}^{(t)} & if -y^{(t)}{\bf w}^T{\bf x}^{(t)}\ge 0 \\
\bf 0 & otherwise
\end{array}
\right.
\end{equation}

- 簡単のため$\eta = 1$として、以上の勾配に基づくパラメータ更新式は以下のようになる

\begin{equation}
{\bf w}^{(t+1)} = \left\{
\begin{array}{ll}
{\bf w}^{(t)}+y^{(t)}{\bf x}^{(t)} & if -y^{(t)}{\bf w}^T{\bf x}^{(t)}\ge 0 \\
{\bf w}^{(t)} & otherwise
\end{array}
\right.
\end{equation}

# 3.6 サポートベクトルマシン

##
- パーセプトロンと同様、線形分類器の1つ

- ハードマージンSVMとソフトマージンSVMの2種類に分けられる

- ソフトマージンSVMは非線形分類にも対応しているので、ハードマージンSVMより実用的

## 3.6.1 線形分離可能とは

##
- $+1$と$-1$のデータの間に分離超平面を引いて全てのデータを正しく分類できる時、データは線形分離可能という

- 具体的には$n=\{1,...,N\}$に対して以下の条件を満たすパラメータが存在するとき、線形分離可能

\begin{equation}
y^{(n)}({\bf w}^T{\bf x}^{(n)}+b)\ge 0
\end{equation}

## 3.6.2 ハードマージンSVMの導出

##
- SVMの基本的な考え方は2つのクラスのできるだけ真ん中に分離超平面を引くということ

- これを実現するためにサポートベクトルとマージンの概念を導入

- 分離超平面を引いたとき、分離超平面から一番近い各クラスのデータ点をサポートベクトル、サポートベクトルから分離超平面までの距離をマージンとする

##
- 線形分類器では識別関数の符号で分類を行うので、SVMの分離超平面は$f({\bf x})=y({\bf w}^T{\bf x}+b)=0$となる

- サポートベクトル${\bf x}^*$と分離超平面$f({\bf x})=0$の距離は次のとおり

\begin{equation}
\frac {|{\bf w}^T{\bf x}^*+b|}{\|{\bf w}\|}
\end{equation}

- 実際には${\bf w}^T{\bf x}^*+b=1$という制約を設けても一般性は失われないので、マージンは次のように書ける

\begin{equation}
\frac {1}{\|{\bf w}\|}
\end{equation}

##
- サポートベクトルは全てのデータ点のうち最も分離超平面に近いデータ点なので、全てのデータ点について以下の制約が存在する

\begin{equation}
|{\bf w}^T{\bf x}^{(t)}+b|\ge 1
\end{equation}

- この制約のもとでマージン最大化問題を解くが、より解きやすくするため、$\|{\bf w}\|^{2}$を最小化する

## 3.6.3 ソフトマージンSVMの導出

##
- 線形分離可能ではないデータに対応するための、ソフトマージン拡張を考える

- ソフトマージンSVMでは制約の代わりに分類失敗の場合はペナルティ$\xi^{(t)}$を与えることとし、ペナルティを目的関数に組み込む

\begin{equation}
L({\bf w})=\min_{{\bf w}}(\sum_{t=1,2,...} \xi^{(t)})+C\|{\bf w}\|^2
\end{equation}

##
- 全ての$\xi^{(t)}$は以下の制約を満たさなければならない

\begin{equation}
\xi^{(t)}=\max(1-y^{(t)}{\bf w}^T{\bf x}^{(t)},0)
\end{equation}

- これを使って目的関数を書き換えると以下のようになる

\begin{equation}
L({\bf w})=\min_{{\bf w}}(\sum_{t=1,2,...} \max(1-y^{(t)}{\bf w}^T{\bf x}^{(t)},0)+C\|{\bf w}\|^2
\end{equation}

- SVMでは損失項はヒンジ損失で、正則化はL2正則化を採用する

## 3.6.4 SVMの目的関数の解釈

##
- 損失項は
$y^{(t)}{\bf w}^T{\bf x}^{(t)}\ge 1$の時0で、それ以外の時は0より大きな値になるので、パーセプトロンより厳格

- また、正則化項を持つことで損失項のみのパーセプトロンと比べて過学習に強くなり、未知データの予測性能が向上

## 3.6.5 確率的勾配降下法によるSVMの学習

##
- SGDによるSVMの学習もパーセプトロンの場合と同様にできる

- 目的関数について場合分けを行うと、勾配は以下のようになる

\begin{equation}
\nabla l({\bf w})= \left\{
\begin{array}{ll}
-y{\bf x}+2C{\bf w} & if -y{\bf w}^T{\bf x}\ge 0 \\
2C{\bf w} & otherwise
\end{array}
\right.
\end{equation}

- 勾配が計算できれば、確率的勾配降下法に当てはめて学習を行う

##
確率的勾配降下法によるSVMの学習

1. **Require**: ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^T{\bf x}^{(t)}\le 1$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1/2)}={\bf w}^{(t)}+\eta^{(t)}y^{(t)}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1/2)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t+1/2)}-2\eta^{(t)}C{\bf w}^{(t)}$
9. **end for**


