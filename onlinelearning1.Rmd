---
title: "オンライン機械学習 勉強会"
subtitle: "chapter 3 基礎の前半"
date: "`r format(Sys.time(),'%Y/%m/%d')`"
output:
  revealjs::revealjs_presentation:
    reveal_option:
      slideNumber: true
    #pandoc_args: [
    #  '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
    #]
    self_contained: True
    center: True
    theme: sky
---

# 3.1 二値分類

```{=html}
<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(revealjs)
library(ggplot2)
library(dplyr)
```

## 

-   入力としてベクトル${\bf x}\in R^m$を受け取り、実数値を返す関数$f({\bf x})$を考える

-   識別境界として$f({\bf x})=0$をとり、$f({\bf x})>0$の時は$1$、それ以外の時は$-1$を返す分類器による分類問題を二値分類という

```{=tex}
\begin{equation}
y=sign(f({\bf x}))
\end{equation}
```
-   例：スパムメールの分類、ある商品を購入するかどうかの予測

# 3.2 線形分類器

## 

- 識別関数$f({\bf x})$を重みベクトル$\bf w$と入力$\bf x$の内積とバイアス項$b$の和で表現する時、この分類器は線形分類器と呼ばれる

\begin{equation}
f({\bf x})={\bf w}^T{\bf x}+b
\end{equation}

- 線形分類器を使って二値分類問題を解くということはパラメータ$\bf w$と$b$をうまく決めるということ

##
- バイアス項$b$については、正規化したり入力$\bf x$を拡張することで省略したりして計算を行う

入力$\bf x$を拡張した場合
\begin{equation}
f({\bf x})={\bf w}^T{\bf x}=0
\end{equation}

重みベクトルのノルム$\|w\|$で正規化して変形した場合
\begin{equation}
f({\bf x})={\bf n}^T{\bf x}-\Delta_{w}={\bf n}^T({\bf x}-{\bf P})=0
\end{equation}
ここで、${\bf n}={\bf w}/\|\bf w\|$、$\Delta_{w}=-b/\|\bf w\|$、$\bf P$は識別関数上の任意の点

- 識別関数を直交するベクトルとして表現して分離超平面に

##
- バイアス項は特殊なので別途最適化した方がよい性能が得られる場合がある

- sgd(stochastic gradient descent)というソルバーではバイアス項の更新のみ学習率を$1/100$にしたヒューリスティックな手法を採用

- 理論的に保証されたアルゴリズムはバイアス付きPassive-Aggressiveアルゴリズム(Tateishi et al., 2013)などがある

# 3.3 パーセプトロン

##
単純パーセプトロンの学習アルゴリズム  

1. **Require:** ${\bf w}^{(1)}=0$
2. **for** $t=1,2,...$ **do**
3. \ \ \ \ **if** $y^{(t)}{\bf w}^{(t)T}{\bf x}^{(t)}\le0$ **then**
4. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}+y^{(t)}{\bf x}^{(t)}$
5. \ \ \ \ **else**
6. \ \ \ \ \ \ \ \ ${\bf w}^{(t+1)}={\bf w}^{(t)}$
7. \ \ \ \ **end if**
8. **end for**

${\bf x}^{(t)}$、$y^{(t)}$は$t$回目に受け取る学習データの組、${\bf w}^{(t+1)}$は$t$回データを受け取った後の重みベクトル

##
- $y^{(t)}{\bf w}^{(t)}{\bf x}^{(t)}\le0$の時、すなわち$t$期のデータが与えられた時に誤分類している場合、次のように内積${\bf w}^T{\bf x}$が変化するように${\bf w}$を更新する

\begin{equation}
{\bf w}^{(t+1)T}{\bf x}^{(t)}-{\bf w}^{(t)T}{\bf x}^{(t)}=y^{(t)}{\bf x}^{(t)T}{\bf x}^{(t)}
\end{equation}

- $y^{(t)}=1$の時は内積は大きくなり、$y^{(t)}=-1$の時は小さくなるので、パラメータ更新により正解に近づいていく

# 3.4 目的関数と最適化手法

## 3.4.1 目的関数の表記

##
- 与えられる学習データを$\{({\bf x}^{(1)},y^{(1)}),({\bf x}^{(2)},y^{(2)}),...\}$とし、便宜上$N$個与えられるとする

- この時、パラメータ$\bf w$で特徴づけられた目的関数$L({\bf w})$を次式で定める

\begin{equation}
L({\bf w})=\sum_{t=1}^N l({\bf x}^{(t)},y^{(t)},{\bf w})+r({\bf w})
\end{equation}

- 右辺第1項は損失項、第2項は正則化項で、アルゴリズムごとに設計する

## 3.4.2 オンライン学習と収束

##
- $N$個の学習データを使ってオンライン学習を行う時、1つずつ入力と正解のラベルを分類器に与えるのを$N$回繰り返すというプロセスではパラメータを収束させるのは難しい

- 同じデータを反復して学習させパラメータの収束をさせるプロセスにおいて、アルゴリズムの性能を向上させるモチベーションが出てくる

- こうした問題設定をIncremental Gradient Methodといい、オンライン学習自体とは区別する場合もある

# 3.5 確率的勾配降下法

## 3.5.1 勾配法とは

##
- 勾配法は関数値と勾配のみを用いて目的関数$L({\bf w})$を最小化する最適化アルゴリズムの総称

- 

## 3.5.2 勾配降下法

## 3.5.3 確率的勾配降下法の詳細

## 3.5.4 パーセプトロンの目的関数

## 3.5.5 パーセプトロンの学習アルゴリズムの導出


# 3.6 サポートベクトルマシン

## 3.6.1 線形分離可能とは

## 3.6.2 ハードマージンSVMの導出

## 3.6.3 ソフトマージンSVMの導出

## 3.6.4 SVMの目的関数の解釈

## 3.6.5 確率的勾配降下法によるSVMの学習

## ふたつめの図

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +
  geom_point()
```

© 2020 GitHub, Inc.
