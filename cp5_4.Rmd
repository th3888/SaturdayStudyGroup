---
title: "chapter 5 Performance Analysis"
date: "2021/6/27"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: "CambridgeUS"
    colortheme: "rose"
header-includes:
  - \usepackage{amsthm}
  - \newtheorem{df}{definition}
  - \newtheorem{thm}{theorem}
  - \newtheorem{lm}{lemma}
  - \newcommand{\argmin}{\mathop{\rm arg~min}\limits}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Regret Analysis

- A player chooses an action ${\theta}^{(t)}\in K$ every $t$ period, where $K$ is a feasible set of actions.

- The cost function $f^{(t)}$ determines the cost $f^{(t)}(\theta^{(t)})$ for action $\theta^{(t)}$.

- The player decides his action based on the strategy.


## Regret Analysis

- How does the player choose an action which minimizes a total cost $\Sigma f^{(t)}(\theta^{(t)})$?

- Can the cost function be minimized even if it is not unknown?

- We introduce a regret about the strategy.

\begin{df}[Regret]
The difference between the total cost of an action based on a strategy $A$ and the total cost of the optimal strategy $\theta^{*}$ is defined as the regret $Regret(A)$ of strategy $A$.
\begin{align}
Regret(A)=\Sigma^{T}_{t=1} f^{(t)}(\theta^{(t)})-\Sigma^{T}_{t=1} f^{(t)}(\theta^{*}) \nonumber
\end{align}
\end{df}


## Regret Analysis

Regret analysis in online learning

- Let action be the parameter of the online learner ${\pmb \theta}^{(t)}\in \mathbb R^{m}$ given the training data $({\pmb x}^{(t)},y^{(t)})$.

- Let the cost function be a loss function $f^{(t)}=({\pmb x}^{(t)},y^{(t)},{\pmb \theta})$.

- In this case, the optimal strategy is the strategy that chooses the action that minimizes the cost function for all training data.


## Follow the Leader

- At first, We consider strategy for choosing action that minimizes the total cost to date.

\begin{align}
{\pmb \theta}^{(t)} = \argmin_{{\pmb \theta}\in K}\Sigma_{i=1}^{t-1} f^{(t)}({\pmb \theta}) \nonumber
\end{align}

- This strategy is called Follow the Leader (FTL).

## Follow the Leader

- However, there are cases where FTL doesn't work.

- Consider action $\theta \in [-1,1]$ and cost function $f^{(t)}(\theta)=(1/2)(-1)^{t}\theta$.

- In this case, the action goes back and forth between $-1$ and $1$ except for the first, as $\theta^{(1)}=0, \theta^{(2)}=-1, \theta^{(2)}=1,...$.

- The cost function is $1/2$ except for the first, as $f^{(1)}(\theta^{(1)})=0,f^{(2)}(\theta^{(2)})=1/2,f^{(3)}(\theta^{(3)})=1/2,...$.


## Follow the Leader

- On the other hand, The optimal strategy is $\theta=0$ and the total cost of it is $0$.
  
- Therefore, FTL regret in this case doesn't approach $0$.

- We have to expand FTL.


## Regularized Follow the Leader

- Regularized Follow the Leader (RFTL)

\begin{align}
{\pmb \theta}^{(t)} = \argmin_{{\pmb \theta}\in K}\eta \Sigma_{i=1}^{t-1} f^{(t)}({\pmb \theta})+R({\pmb \theta}) \nonumber
\end{align}

- Let $R({\pmb \theta})$ be convex regularization function. Let $\eta\ge 0$ be parameter that determines the degree of regularization.

- When choosing the first action, the cost function is not presented, so action is determined only by the regularization term.

\begin{align}
{\pmb \theta}^{(1)} = \argmin_{{\pmb \theta}\in K}R({\pmb \theta}) \nonumber
\end{align}

## Regularized Follow the Leader

- We introduce lemma and definitions to derive RFTL regret.

\begin{lm}
For any vector ${\pmb u}\in K$, the following holds.
\begin{align}
\Sigma_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le \Sigma_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{t+1})+\frac{1}{\eta}\left\{R({\pmb u})-R({\pmb \theta}^{(1)})\right\}
\end{align}

\end{lm}


## Regularized Follow the Leader

\begin{proof}

For simplicity, let us assume that ${\pmb f}^{(0)}=\frac{1}{\eta}R({\pmb \theta})$ and the algorithm starts at $t=0$.

\begin{align}
\Sigma_{t=0}^{T}{\pmb f}^{(t)}({\pmb \theta}) = \Sigma_{t=1}^{T}{\pmb f}^{(t)}({\pmb \theta}) + \frac{1}{\eta}R({\pmb \theta}) \nonumber
\end{align}

In this time, the lemma can be expressed as following.
\begin{align}
\Sigma_{t=0}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le \Sigma_{t=0}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}) \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}


## Regularized Follow the Leader

\begin{proof}
At $t=0$,

by definition, ${\pmb \theta}^{(1)}=\argmin_{\pmb \theta} R({\pmb \theta})$ and ${\pmb f}^{(0)}({\pmb \theta}^{(1)})\le {\pmb f}^{(0)}({\pmb u})$ holds.

therefore, 
\begin{align}
{\pmb f}^{(0)}({\pmb \theta}^{(0)})-{\pmb f}^{(0)}({\pmb u})\le {\pmb f}^{(0)}({\pmb \theta}^{(0)})-{\pmb f}^{(0)}({\pmb \theta}^{(1)}) \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}


## Regularized Follow the Leader

\begin{proof}

At $t>0$,

assume that lemma holds for $t=T$.

In this time, 
\begin{align}
{\pmb \theta}^{(T+2)}=\argmin_{{\pmb \theta}}\Sigma_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta})
\end{align}
\begin{align}
{\pmb \theta}^{(T+1)}=\argmin_{{\pmb \theta}}\Sigma_{t=0}^{T}{\pmb f}^{(t)}({\pmb \theta})
\end{align}


\end{proof}

