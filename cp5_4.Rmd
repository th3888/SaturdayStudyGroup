---
title: "chapter 5 性能解析"
date: "2021/6/16"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: "CambridgeUS"
    colortheme: "rose"
mainfont: ipaexg.ttf
header-includes:
  - \usepackage{amsthm}
  - \newtheorem{df}{定義}
  - \newtheorem{thm}{定理}
  - \newtheorem{lm}{補題}
  - \newcommand{\argmin}{\mathop{\rm arg~min}\limits}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## リグレット解析

- プレイヤーは$t$期ごとに実行可能なアクション集合に含まれる
1つのアクション$\theta^{(t)} \in K$を選択

- コスト関数$f^{(t)}$によりアクション$\theta^{(t)}$に対するコスト
$f^{(t)}(\theta^{(t)})$が定まる

- プレイヤーは戦略に基づいてアクションを決定する


## リグレット解析

- プレイヤーはどのように戦略を選んで合計コスト
$\Sigma f^{(t)}(\theta^{(t)})$を最小化するか

- そもそもコスト関数$f^{(t)}$がわからない場合でも最小化できるか

- ここで戦略のリグレットを導入する

\begin{df}[戦略のリグレット]
ある戦略$A$に基づくアクションの合計コストと最適戦略$\theta^{*}$
による合計コストの差を戦略$A$のリグレット$Regret(A)$と定義する。
\begin{align}
Regret(A)=\Sigma^{T}_{t=1} f^{(t)}(\theta^{(t)})-\Sigma^{T}_{t=1} f^{(t)}(\theta^{*}) \nonumber
\end{align}
\end{df}

## リグレット解析

リグレット解析の意味付け


- $Regret(A)$が$T$についての線形な関数ならばコスト差は
縮まらない

- $Regret(A)$が$T$についての線形な関数より小さければ、
コストの差は0に近づいていく

- このとき、その戦略$A$が達成するコストは最適戦略の
コストに限りなく近づいていく


## リグレット解析

オンライン学習におけるリグレット解析


- 学習データ$({\pmb x}^{(t)},y^{(t)})$が与えられたときの
オンライン学習器のパラメータ
${\pmb \theta}^{(t)}\in \mathbb R^{m}$をアクションとする

- 損失関数$f^{(t)}=({\pmb x}^{(t)},y^{(t)},{\pmb \theta})$
をコスト関数とする

- この場合、最適戦略はすべての学習データに対するコスト関数を
最小にするアクションを選ぶ戦略となる


## Follow the Leader

- 単純な戦略として、これまでの合計コストを最小にするような
アクションを選ぶものを考える

\begin{align}
{\pmb \theta}^{(1)} = \argmin_{{\pmb \theta}\in K}\Sigma_{i=1}^{t-1} f^{(t)}({\pmb \theta}) \nonumber
\end{align}

- この戦略のことをFollow the Leader(FTL)という

## Follow the Reader

- FTLではうまくいかないケースがある

- アクション$\theta \in [-1,1]$、コスト関数$f^{(t)}(\theta)=(1/2)(-1)^{t}\theta$
を考える

- このとき、アクションは$\theta^{(1)}=0, \theta^{(2)}=-1, \theta^{(2)}=1,...$と最初を除き$-1$と$1$を行き来する

- コスト関数は$f^{(1)}(\theta^{(1)})=0,f^{(2)}(\theta^{(2)})=1/2,f^{(3)}(\theta^{(3)})=1/2,...$と最初を除き$1/2$となる

## Follow the Leader

- 一方、最適戦略は$\theta=0$であり、合計コストは$0$

- よって、この場合のFTLのリグレットは$T$に対して線形な関数で
最適戦略に近づくことはない

- FTLの拡張を考える必要がある

## Regularized Follow the Reader

- Regularized Follow the Reader(RFTL)

\begin{align}
{\pmb \theta}^{(1)} = \argmin_{{\pmb \theta}\in K}\eta \Sigma_{i=1}^{t-1} f^{(t)}({\pmb \theta})+R({\pmb \theta}) \nonumber
\end{align}

- $R({\pmb \theta})$は凸な正則化関数、$\eta\ge 0$は正則化の程度
を定めるパラメータ

- 最初のアクションを定めるとき、コスト関数が提示されていないので
正則化項のみで定める

\begin{align}
{\pmb \theta}^{(1)} = \argmin_{{\pmb \theta}\in K}R({\pmb \theta}) \nonumber
\end{align}

## Regularized Follow the Reader

- RFTLのリグレットを導入するために補題と定義を導入する

\begin{lm}
任意のベクトル${\pmb u}\in K$について、次が成り立つ。
\begin{align}
\Sigma_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le \Sigma_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{t+1})+\frac{1}{\eta}(R({\pmb u})-R({\pmb \theta}^{(1)}))
\end{align}

\end{lm}


## Regularized Follow the Reader

\begin{proof}

${\pmb f}^{(0)}=\frac{1}{\eta}R({\pmb \theta})$とし、
アルゴリズムは$t=0$から始まるものとすると、

\begin{align}
\Sigma_{t=0}^{T}{\pmb f}^{(t)}({\pmb \theta}) = \Sigma_{t=1}^{T}{\pmb f}^{(t)}({\pmb \theta}) + \frac{1}{\eta}R({\pmb \theta}) \nonumber
\end{align}

このとき、補題は
\begin{align}
\Sigma_{t=0}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le \Sigma_{t=0}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{t+1}) \nonumber
\end{align}
と表現できる。

\let\qedsymbol\relax
\end{proof}


## Regularized Follow the Reader

\begin{proof}
$t=0$のとき

定義より${\pmb \theta}^{(1)}=\argmin_{\pmb \theta} R({\pmb \theta})$であり、
${\pmb f}^{(0)}({\pmb \theta}^{(1)})\le {\pmb f}^{(0)}({\pmb u})$が成り立つ。

よって、
\begin{align}
{\pmb f}^{(0)}({\pmb \theta}^{(0)})-{\pmb f}^{(0)}({\pmb u})\le {\pmb f}^{(0)}({\pmb \theta}^{(0)})-{\pmb f}^{(0)}({\pmb \theta}^{(1)}) \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}

## Regularized Follow the Reader

\begin{proof}

$t>0$のとき

$t=T$で補題が成り立っていると仮定する。

このとき、
\begin{align}
{\pmb \theta}^{(T+2)}=\argmin_{{\pmb \theta}}\Sigma_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta})\nonumber
\end{align}
\begin{align}
{\pmb \theta}^{(T+1)}=\argmin_{{\pmb \theta}}\Sigma_{t=0}^{T}{\pmb f}^{(t)}({\pmb \theta})\nonumber
\end{align}

\end{proof}

