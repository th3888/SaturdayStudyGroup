---
title: "chapter 5 Performance Analysis"
date: "2021/7/3"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: "CambridgeUS"
    colortheme: "rose"
header-includes:
  - \usepackage{amsthm}
  - \newtheorem{df}{definition}
  - \newtheorem{thm}{theorem}
  - \newtheorem{lm}{lemma}
  - \newcommand{\argmin}{\mathop{\rm arg~min}\limits}
  - \newtheorem{alg}{algorithm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Regret Analysis

- A player chooses an action ${\theta}^{(t)}\in K$ every $t$ period, where $K$ is a feasible set of actions.

- The cost function $f^{(t)}$ determines the cost $f^{(t)}(\theta^{(t)})$ for action $\theta^{(t)}$.

- The player decides his action based on the strategy.


## Regret Analysis

- How does the player choose an action which minimizes a total cost $\sum f^{(t)}(\theta^{(t)})$?

- Can the cost function be minimized even if it is not unknown?

- We introduce a regret about the strategy.

\begin{df}[Regret]
The difference between the total cost of an action based on a strategy $A$ and the total cost of the optimal strategy $\theta^{*}$ is defined as the regret $Regret(A)$ of strategy $A$.
\begin{align}
Regret(A)=\sum^{T}_{t=1} f^{(t)}(\theta^{(t)})-\sum^{T}_{t=1} f^{(t)}(\theta^{*}) \nonumber
\end{align}
\end{df}


## Regret Analysis

Regret analysis in online learning

- Let action be the parameter of the online learner ${\pmb \theta}^{(t)}\in \mathbb R^{m}$ given the training data $({\pmb x}^{(t)},y^{(t)})$.

- Let the cost function be a loss function $f^{(t)}=({\pmb x}^{(t)},y^{(t)},{\pmb \theta})$.

- In this case, the optimal strategy is the strategy that chooses the action that minimizes the cost function for all training data.


## Follow the Leader

- At first, We consider strategy for choosing action that minimizes the total cost to date.

\begin{align}
{\pmb \theta}^{(t)} = \argmin_{{\pmb \theta}\in K}\sum_{i=1}^{t-1} f^{(t)}({\pmb \theta}) \nonumber
\end{align}

- This strategy is called Follow the Leader (FTL).

## Follow the Leader

- However, there are cases where FTL doesn't work.

- Consider action $\theta \in [-1,1]$ and cost function $f^{(t)}(\theta)=(1/2)(-1)^{t}\theta$.

- In this case, the action goes back and forth between $-1$ and $1$ except for the first, as $\theta^{(1)}=0, \theta^{(2)}=-1, \theta^{(2)}=1,...$.

- The cost function is $1/2$ except for the first, as $f^{(1)}(\theta^{(1)})=0,f^{(2)}(\theta^{(2)})=1/2,f^{(3)}(\theta^{(3)})=1/2,...$.


## Follow the Leader

- On the other hand, The optimal strategy is $\theta=0$ and the total cost of it is $0$.
  
- Therefore, FTL regret in this case doesn't approach $0$.

- We have to expand FTL.


## Regularized Follow the Leader

- Regularized Follow the Leader (RFTL)

\begin{align}
{\pmb \theta}^{(t)} = \argmin_{{\pmb \theta}\in K}\eta \sum_{i=1}^{t-1} f^{(t)}({\pmb \theta})+R({\pmb \theta}) \nonumber
\end{align}

- Let $R({\pmb \theta})$ be convex regularization function. Let $\eta\ge 0$ be parameter that determines the degree of regularization.

- When choosing the first action, the cost function is not presented, so action is determined only by the regularization term.

\begin{align}
{\pmb \theta}^{(1)} = \argmin_{{\pmb \theta}\in K}R({\pmb \theta}) \nonumber
\end{align}

## Regularized Follow the Leader

- We introduce lemma and definitions to derive RFTL regret.

\begin{lm}
For any vector ${\pmb u}\in K$, the following holds.
\begin{align}
\sum_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le \sum_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})+\frac{1}{\eta}\left\{R({\pmb u})-R({\pmb \theta}^{(1)})\right\}
\end{align}

\end{lm}


## Regularized Follow the Leader

\begin{proof}

For simplicity, let us assume that ${\pmb f}^{(0)}=\frac{1}{\eta}R({\pmb \theta})$ and the algorithm starts at $t=0$.

\begin{align}
\sum_{t=0}^{T}{\pmb f}^{(t)}({\pmb \theta}) = \sum_{t=1}^{T}{\pmb f}^{(t)}({\pmb \theta}) + \frac{1}{\eta}R({\pmb \theta}) \nonumber
\end{align}

In this time, the lemma can be expressed as following.
\begin{align}
\sum_{t=0}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le \sum_{t=0}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}) \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}


## Regularized Follow the Leader

\begin{proof}
At $t=0$,

by definition, ${\pmb \theta}^{(1)}=\argmin_{\pmb \theta} R({\pmb \theta})$ and ${\pmb f}^{(0)}({\pmb \theta}^{(1)})\le {\pmb f}^{(0)}({\pmb u})$ holds.

therefore, 
\begin{align}
{\pmb f}^{(0)}({\pmb \theta}^{(0)})-{\pmb f}^{(0)}({\pmb u})\le {\pmb f}^{(0)}({\pmb \theta}^{(0)})-{\pmb f}^{(0)}({\pmb \theta}^{(1)}) \nonumber
\end{align}

\let\qedsymbol\relax
\end{proof}


## Regularized Follow the Leader

\begin{proof}

At $t>0$,

assume that lemma holds for $t=T$.

In this time, 
\begin{align}
{\pmb \theta}^{(T+2)}=\argmin_{{\pmb \theta}}\sum_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta})
\end{align}
\begin{align}
{\pmb \theta}^{(T+1)}=\argmin_{{\pmb \theta}}\sum_{t=0}^{T}{\pmb f}^{(t)}({\pmb \theta})
\end{align}


\end{proof}


## Regularized Follow the Leader

\begin{proof}

Using equation $(2)$ and $(3)$,

\begin{align}
&\sum_{t=0}^{T+1}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \nonumber\\
&\le \sum_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta}^{(t)}) - \sum_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta}^{(T+2)}) \nonumber\\
&= \sum_{t=0}^{T}({\pmb f}^{(t)}({\pmb \theta}^{(t)})-{\pmb f}^{(t)}({\pmb \theta}^{(T+2)}))+{\pmb f}^{(T+1)}({\pmb \theta}^{(T+1)})-{\pmb f}^{(T+1)}({\pmb \theta}^{(T+2)}) \nonumber\\
&\le \sum_{t=0}^{T}({\pmb f}^{(t)}({\pmb \theta}^{(t)})-{\pmb f}^{(t)}({\pmb \theta}^{(T+1)}))+{\pmb f}^{(T+1)}({\pmb \theta}^{(T+1)})-{\pmb f}^{(T+1)}({\pmb \theta}^{(T+2)}) \nonumber\\
&= \sum_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta}^{(t)})-{\pmb f}^{(t)}({\pmb \theta}^{(t+1)}) \nonumber\\
&= \sum_{t=0}^{T+1}{\pmb f}^{(t)}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}) \nonumber
\end{align}
\end{proof}


## Regularized Follow the Leader

\begin{df}[norm based on positive semi-definite matrix]
We define $\|{\pmb x}\|_{{\pmb A}}=\sqrt{{\pmb x}^{T}{\pmb A}{\pmb x}}$ as the norm of vector $\pmb x$ based on positve semi-definite matrix $\pmb A$.

We also define $\|{\pmb x}\|_{{\pmb A}^{-1}}=\|{\pmb x}\|^{*}_{\pmb A}$ as a dual norm. 
\end{df}

- In this time, from generalized Cauchy-Schwarz inequality, the following holds.

\begin{align}
{\pmb x}^{T}{\pmb y}\le \|{\pmb x}\|_{\pmb A}\|{\pmb y}\|^{*}_{\pmb A} \nonumber
\end{align}


## Regularized Follow the Leader

\begin{df}[norm of cost function]
A norm of cost function measured by the regularization function is difined as

\begin{align}
\lambda = \max_{t,{\pmb \theta}\in K}{\pmb f}^{(t)T}\{\nabla^{2}R({\pmb \theta})\}^{-1}{\pmb f}^{(t)} \nonumber
\end{align}

\end{df}


## Regularized Follow the Leader

\begin{df}[diameter of feasible area]
A diameter measured by the regularization function is defined as

\begin{align}
D=\max_{{\pmb \theta}\in K}R({\pmb \theta})-R({\pmb \theta}^{(1)})\nonumber
\end{align}
\end{df}


## Regularized Follow the Leader

\begin{thm}[regret of RFTL]
RFTL achives the following regret for any vector ${\pmb u}\in K$.

\begin{align}
Regret(A) = \sum_{t=1}^{T}{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb u}) \le 2\sqrt{2\lambda DT} \nonumber
\end{align}
\end{thm}


## Regularized Follow the Leader

\begin{proof}

At first, we define $\Phi$ as following.

\begin{align}
\Phi^{(t)}({\pmb \theta})=\eta\sum_{i=1}^{t}f^{(i)}({\pmb \theta})+R({\pmb \theta})\nonumber
\end{align}

By Taylor expansion of $\Phi^{(t)}$ around ${\pmb \theta}^{(t+1)}$ and using intermediate value theorem, we can show following.

\begin{align}
\Phi^{(t)}({\pmb \theta}^{(t)})&=\Phi^{(t)}({\pmb \theta}^{(t+1)})+({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})^{T}\nabla\Phi^{(t)}({\pmb \theta}^{(t+1)})\nonumber\\
&+\frac{1}{2}\|{\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}\|^{2}_{{\pmb z}^{(t)}}\nonumber\\
&\ge \Phi^{(t)}({\pmb \theta}^{(t+1)})+\frac{1}{2}\|{\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}\|^{2}_{{\pmb z}^{(t)}}\nonumber
\end{align}
\let\qedsymbol\relax
\end{proof}


## Regularized Follow the Leader

\begin{proof}
Here, from intermediate value theorem, ${\pmb z}^{(t)}\in [{\pmb \theta}^{(t+1)},{\pmb \theta}^{(t)}]$

This inequality holds because ${\pmb \theta}^{(t)}$ achieves the minimum value of $\Phi^{(t)}$.

By transforming the equation, we can get following.

\begin{align}
\|{\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}\|^{2}_{{\pmb z}^{(t)}} &\le 2(\Phi^{(t)}({\pmb \theta}^{(t)})-\Phi^{(t)}({\pmb \theta}^{(t+1)}))\nonumber\\
&=2(\Phi^{(t-1)}({\pmb \theta}^{(t)})-\Phi^{(t-1)}({\pmb \theta}^{(t+1)}))+2\eta{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})\nonumber\\
&\le 2\eta{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})\nonumber
\end{align}

\end{proof}


## Regularized Follow the Leader

\begin{proof}
Also, using generalized Cauchy-Schwarz inequality, the following holds.

\begin{align}
{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})&\le \|{\pmb f}^{(t)}\|^{*}_{\pmb z}\|{\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)}\|_{\pmb z}\nonumber\\
&\le \|{\pmb f}^{(t)}\|^{*}_{\pmb z}\sqrt{2\eta{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})}\nonumber
\end{align}

furthermore,

\begin{align}
{\pmb f}^{(t)T}({\pmb \theta}^{(t)}-{\pmb \theta}^{(t+1)})\le 2\eta \|{\pmb f}^{(t)}\|^{*2}_{\pmb z}\le 2\eta\lambda 
\end{align}
\end{proof}

## Regularized Follow the Leader

\begin{proof}
Using $(1)$ and $(4)$, adding up to $T$ and letting $\eta$ be $\eta = \sqrt{\frac{D}{2\lambda T}}$ which minimizes this equation, we can get

\begin{align}
Regret(A)\le min_{\eta}\left[2\eta\lambda T + \frac{1}{\eta}\{R({\pmb u})-R({\pmb \theta}^{(t)})\}\right]\le 2\sqrt{2D\lambda T}\nonumber
\end{align}
\end{proof}

## Regret with strong convex loss function
cf. Hazan, Agarwal and Kale (2007) 

- a player chooses a point (action) from a set in Euclidean space denoted $K\subseteq \mathbb R^{n}$.

- We assume that the set $K$ is non-empty, bounded, closed and convex.

- We denote the number of iterations by $T$ which is unknown to the player.


## Regret with strong convex loss function

- At iteration $t$, the player chooses ${\pmb \theta}_{t}\in K$.

- After committing to this choice, a convex cost function $f_{t}:K\mapsto \mathbb R$ is revealed.


## Regret with strong convex loss function

- Consider a player using an algorithm for online game playing (strategy) $A$.

- At iteration $t$, the algorithm takes as input the history of cost function $f_{1},...,f_{t-1}$ and produces a feasible point $A(\{f_{1},...,f_{t-1}\})$ in the domain $K$.

- When there is no ambiguity, we simply denote ${\pmb \theta}_{t}=A(\{f_{1},...,f_{t-1}\})$.

## Regret with strong convex loss function

- The regret of the online player using algorithm $A$ at time $T$ is defined to be the total cost minus the cost of the best single decision, where the best is chosen with the benefit of hindsight.

\begin{align}
Regret(A, \{f_{1},...,f_{T}\})=E\left\{\sum_{t=1}^{T}f_{t}({\pmb \theta}_{t}) \right\}-\min_{{\pmb \theta}\in K}\sum_{t=1}^{T}f_{t}({\pmb \theta}).\nonumber
\end{align}

- We are usually interested in an upper bound on the worst case guaranteed regret, denoted

\begin{align}
Regret_{T}(A)=\sup_{\{f_{1},...,f_{T}\}} \{Regret(A, \{f_{1},...,f_{T}\})\}.\nonumber
\end{align}

## Regret with strong convex loss function

\begin{df}
We say that the cost functions have gradients upper bounded by a number $G$ if the following holds:

\begin{align}
\sup_{{\pmb \theta}\in K, t\in\{1,...,T\}}\|\nabla f_{t}({\pmb \theta})\|_{2}\le G.\nonumber
\end{align}
\end{df}

## Regret with strong convex loss function

\begin{df}
We say that the Hessian of all cost functions is lower bounded by a number $\alpha>0$, if the following holds:

\begin{align}
\forall {\pmb \theta}\in K,t\in \{f_{1},...,f_{T}\}: \nabla^{2}f_{t}({\pmb \theta})\succeq \alpha{\pmb I}_{n}.\nonumber
\end{align}

$\alpha$ is a lower bound on eigenvalues of all the Hessians of the constraints at all points in the domain.
Such function is called $\alpha$-strong convex.
\end{df}

## Regret with strong convex loss function

\begin{alg}[Online Gradient Descent]
Inputs: convex set $K\subset \mathbb R^{n}$, step sizes $\eta_{1},\eta_{2},...\ge0$, inital ${\pmb \theta}_{1}\in K$.

- In iteration $1$, use ${\pmb \theta}_{1}\in K$.

- In iteration $t>1$, use

\begin{align}
{\pmb \theta}_{t}=\pi_{K}({\pmb \theta}_{t-1}-\eta_{t-1}\nabla f_{t-1}({\pmb \theta}_{t-1})).\nonumber
\end{align}

Here, $\pi_{K}$ denotes the projection onto nearest point in $K$, $\pi_{K}({\pmb y})=\argmin_{{\pmb \theta}\in K}\|{\pmb \theta}-{\pmb y}\|_{2}$.
\end{alg}

## Regret with strong convex loss function

\begin{df}
For a given positive semi-definite matrix ${\pmb A}$, a generalized projection of ${\pmb y}\in \mathbb R^{n}$ onto the convex set $K$ is defined as

\begin{align}
\pi^{\pmb A}_{K}(\pmb y)=\argmin_{{\pmb \theta}\in K}({\pmb \theta}-{\pmb y})^{\top}{\pmb A}({\pmb \theta}-{\pmb y}).\nonumber
\end{align}

Thus, the Euclidean projection can be seen to be a generalized projection with ${\pmb A}={\pmb I}_{n}$.
\end{df}


## Regret with strong convex function

\begin{lm}
Let $K\subseteq\mathbb R^{n}$ be a convex set, ${\pmb y}\in \mathbb R^{n}$ and ${\pmb z}=\pi^{\pmb A}_{K}({\pmb y})$ be the generalized projection of ${\pmb y}$ onto $K$ according to positive semi-definite matrix ${\pmb A}$.
Then for any point ${\pmb a}\in K$ it holds that

\begin{align}
({\pmb y}-{\pmb a})^{\top}{\pmb A}({\pmb y}-{\pmb a})\ge ({\pmb z}-{\pmb a})^{\top}{\pmb A}({\pmb z}-{\pmb a}).\nonumber
\end{align}
\end{lm}

## Regret with strong convex loss function

\begin{proof}
By definition of generalized projections, the point $\pmb z$ minimizes the function $f(\pmb \theta)=({\pmb \theta}-{\pmb y})^{\top}{\pmb A}({\pmb \theta}-{\pmb y})$ over the convex set.
It is a well known fact in optimization that for the optimum $\pmb z$ the following holds.

\begin{align}
\forall {\pmb a}\in K: \nabla f(\pmb z)^{\top}({\pmb a}-{\pmb z})\ge 0\nonumber
\end{align}

which implies

\begin{align}
2({\pmb z}-{\pmb y})^{\top}{\pmb A}({\pmb a}-{\pmb z})\ge 0\Rightarrow 2{\pmb a}^{\top}{\pmb A}({\pmb z}-{\pmb y})\ge 2{\pmb z}^{\top}{\pmb A}({\pmb z}-{\pmb y}).\nonumber
\end{align}
\end{proof}

## Regret with strong convex loss function

\begin{proof}
Now by simple calculation:

\begin{align}
({\pmb y}-{\pmb a})^{\top}{\pmb A}({\pmb y}-{\pmb a})-({\pmb z}-{\pmb a})^{\top}{\pmb A}({\pmb z}-{\pmb a})&=\pmb y^{\top}{\pmb A}{\pmb y}-\pmb z^{\top}{\pmb A}{\pmb z}+2{\pmb a}^{\top}{\pmb A}({\pmb z}-{\pmb y})\nonumber\\
&\ge \pmb y^{\top}{\pmb A}{\pmb y}-\pmb z^{\top}{\pmb A}{\pmb z}+2{\pmb z}^{\top}{\pmb A}({\pmb z}-{\pmb y})\nonumber\\
&= \pmb y^{\top}{\pmb A}{\pmb y}-2\pmb z^{\top}{\pmb A}{\pmb y}+\pmb z^{\top}{\pmb A}{\pmb z}\nonumber\\
&=({\pmb z}-{\pmb y})^{\top}{\pmb A}({\pmb z}-{\pmb y})\ge 0.\nonumber
\end{align}
\end{proof}


## Regret with strong convex loss function

\begin{thm}
Online Gradient Descent with step sizes $\eta_{t}=\frac{1}{\alpha t}$ achives the following guarantee, for all $T\ge 1$

\begin{align}
Regret_{T}(OGD)\le \frac{G^{2}}{2\alpha}(1+\log{T}).\nonumber
\end{align}

\end{thm}

## Regret with strong convex loss function

\begin{proof}
Let ${\pmb \theta}^{*}=\argmin \sum^{T}_{t=1}f_{t}({\pmb \theta})$.
Recall the difinition of regret

\begin{align}
Regret_{T}(OGD) = \sum^{T}_{t=1}f_{t}({\pmb \theta}_{t})-\sum^{T}_{t=1}f_{t}({\pmb \theta}^{*}).\nonumber
\end{align}

Define $\nabla_{t}=\nabla f_{t}({\pmb \theta}_{t})$.
By using the Taylor series approximation, we have, for some point $\xi_{t}$ on the line segment joining $\pmb \theta_{t}$ to $\pmb \theta^{*}$,

\begin{align}
f_{t}({\pmb \theta}^{*})&=f_{t}({\pmb \theta}_{t})+\nabla^{\top}_{t}({\pmb \theta}^{*}-{\pmb \theta}_{t})+\frac{1}{2}({\pmb \theta}^{*}-{\pmb \theta}_{t})^{\top}\nabla^{2}f_{t}(\xi_{t})({\pmb \theta}^{*}-{\pmb \theta}_{t})\nonumber\\
&\ge f_{t}({\pmb \theta}_{t})+\nabla^{\top}_{t}({\pmb \theta}^{*}-{\pmb \theta}_{t})+\frac{\alpha}{2}\|{\pmb \theta}^{*}-{\pmb \theta}_{t}\|^{2}.\nonumber
\end{align}

\end{proof}

## Regret with strong convex loss function

\begin{proof}

The inequality follows from $\alpha$-strong convexity.
Thus, we have

\begin{align}
2\{f_{t}({\pmb \theta}_{t})-f_{t}({\pmb \theta}^{*})\}\le 2\nabla^{\top}({\pmb \theta}_{t}-{\pmb \theta}^{*})-\alpha\|{\pmb \theta}^{*}-{\pmb \theta}_{t}\|^{2}.
\end{align}

Using the update rule for $\pmb \theta_{t+1}$ and lemma, we get

\begin{align}
\|{\pmb \theta}_{t+1}-{\pmb \theta^{*}}\|^{2}&=\|\pi({\pmb \theta}_{t}-\eta_{t}\nabla_{t})-{\pmb \theta}^{*}\|^{2}\nonumber\\
&\le \|{\pmb \theta}_{t}-\eta_{t}\nabla_{t}-{\pmb \theta}^{*}\|^{2}.\nonumber
\end{align}
\end{proof}

## Regret with strong convex loss function

\begin{proof}

Hence,

\begin{align}
&\|{\pmb \theta}_{t+1}-{\pmb \theta^{*}}\|^{2}\le \|{\pmb \theta}_{t}-{\pmb \theta^{*}}\|^{2}+\eta^{2}_{t}\|\nabla_{t}\|^{2}-2\eta_{t}\nabla^{\top}_{t}({\pmb \theta}_{t}-{\pmb \theta^{*}}),\nonumber\\
&2\nabla^{\top}_{t}({\pmb \theta}_{t}-{\pmb \theta^{*}})\le \frac{\|{\pmb \theta}_{t}-{\pmb \theta^{*}}\|^{2}-\|{\pmb \theta}_{t+1}-{\pmb \theta^{*}}\|^{2}}{\eta_{t}}+\eta_{t}G^{2}.
\end{align}

\end{proof}

## Regret with strong convex loss function

\begin{proof}

Sum up $(6)$ from $t=1$ to $T$ and using $(5)$, we have:

\begin{align}
2\sum^{T}_{t=1}f_{t}({\pmb \theta}_{t})-f_{t}({\pmb \theta}^{*})&\le \sum^{T}_{t=1}\|{\pmb \theta}_{t}-{\pmb \theta^{*}}\|^{2}\left(\frac{1}{\eta_{t-1}}+\frac{1}{\eta_{t}}-\alpha\right)+G^{2}\sum_{t=1}^{T}\eta_{t}\nonumber\\
&=G^{2}\sum^{T}_{t=1}\frac{1}{\alpha t}\nonumber\\
&\le \frac{G^{2}}{\alpha}(1+\log{T}).\nonumber
\end{align}

\end{proof}

## Regret for SSP

cf. Ho et al. (2013)

- Let $\pmb w_{1}$ be a initial value of a parameter voctor, $\pmb u_{p}^{t}$ be a difference vector of process $p\in\{1,...,P\}$ in the $t$th update and $\tilde{\pmb w}_{p}^{T}$ be a noisy state read by process $p$ at iteration $T$.

\begin{df}[bounded staleness condition]
Fix a staleness $s$. 
Then the noisy state $\tilde{\pmb w}_{p}^{T}$ is equals to

\begin{align}
\tilde{\pmb w}_{p}^{T}=\pmb w_{1}+\sum_{t=1}^{T-s-1}\sum_{q=1}^{P}{\pmb u}_{q}^{t}+\sum_{t=T-s}^{T-1}{\pmb u}_{p}^{t}+\sum_{(q,t)\in S_{p}^{T}}{\pmb u}_{q}^{t},
\end{align}

\end{df}

## Regret for SSP

where $S_{p}^{T}\subseteq W_{p}^{T}=(\{1,..,P\}\backslash \{p\})Ã—\{T-s,...,T+s-1\}$ is some subset of the updates $\pmb u$ written in the width-$2s$ "window" $W_{p}^{T}$, which ranges from iteration $T-s$ to $T+s-1$ and does not include updates from process $p$.


## Regret for SSP
- In other words, the noisy state $\tilde{\pmb w}_{p}^{T}$ consists of three parts:

1. Guaranteed "pre-window" updates from iteration $1$ to $T-s-1$, over the all processes. 

2. Guaranteed "read-my-writes" set $\{(p, T-s),...,(p,T-1)\}$ that covers all "in-window" updates made by the querying process $p$.

3. Guaranteed "in-window" updates $S_{p}^{T}$ from the width-$2s$ window $\{T-s,...,T+s-1\}$ (not counting updates from process $p$).


## Regret for SSP

- Under $s=0$, SSP reduces BSP.

\begin{proof}
$s=0$ implies $[T, T-s-1]=\emptyset$, and therefore $\tilde{\pmb w}_{p}^{T}$ exactly consists of all updates until clock $T-1$.
\end{proof}

## Regret for SSP

- Our key tool for regret analysis is to define a reference sequence of state ${\pmb w}_{t}$, informally refered to as the "true" sequence:

\begin{align}
{\pmb w}_{T}={\pmb w}^{1}
\end{align}